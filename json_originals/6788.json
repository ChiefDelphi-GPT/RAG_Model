{"data":"{\"post_stream\":{\"posts\":[{\"id\":3364383,\"name\":\"\",\"username\":\"SirFire\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/sirfire/{size}/222590_2.png\",\"created_at\":\"2024-04-17T17:42:32.341Z\",\"cooked\":\"\\u003cp\\u003eHi \\u003cspan class=\\\"abbreviation\\\"\\u003eCD\\u003ctemplate class=\\\"tooltiptext\\\" data-text=\\\"Chief Delphi\\\"\\u003e\\u003c/template\\u003e\\u003c/span\\u003e,\\u003c/p\\u003e\\n\\u003cp\\u003eI looked briefly for any other threads with this topic, but didn’t find any. Please correct me if I am mistaken.\\u003c/p\\u003e\\n\\u003cp\\u003eI’d like to start a discussion about the applications of more complex vision algorithms and possibly artificial intelligence during the TELEOP phase of \\u003cspan class=\\\"abbreviation\\\"\\u003eFRC\\u003ctemplate class=\\\"tooltiptext\\\" data-text=\\\"FIRST Robotics Competition\\\"\\u003e\\u003c/template\\u003e\\u003c/span\\u003e matches, and beyond. I am primarily considering swerve drive in my thoughts, firstly because we have used swerve drive successfully for the first time this year and secondly because it is becoming the most common drive train.\\u003c/p\\u003e\\n\\u003cp\\u003eI’m certain that I’m not the first person to think about this kind of thing, but looking briefly into the kind of technology integrated into \\u003ca href=\\\"https://www.tesla.com/autopilot\\\" rel=\\\"noopener nofollow ugc\\\"\\u003eTesla Autopilot\\u003c/a\\u003e by which the car can quickly and completely autonomously make accurate decisions based on many factors in a variety of environments, it inspired an idea in my mind in which a robot makes decisions on the field based on, firstly where it knows it cannot go, (we already have this in the form of PathPlanner’s on-the-fly path generation) and secondly where it determines that other robots are.\\u003c/p\\u003e\\n\\u003cp\\u003eI have not personally seen examples of teams being able to determine an accurate position of each robot around it (within a useable distance), but if any have, please let me know! I’d love to look into it.\\u003c/p\\u003e\\n\\u003cp\\u003eThe two primary applications that I considered for such an idea are:\\u003c/p\\u003e\\n\\u003col\\u003e\\n\\u003cli\\u003eUsing the position of other robots to autonomously move around them, thereby decreasing cycle time, and\\u003c/li\\u003e\\n\\u003cli\\u003eusing the position of other robots to autonomously play effective and legal defense.\\u003c/li\\u003e\\n\\u003c/ol\\u003e\\n\\u003cp\\u003eWhile in one sense I personally am super interested in the development of this, I am also concerned about the adverse issues it may cause and that it seems inevitable. My main concern is that, if such technology becomes more common and/or available for purchase, students will no longer need to learn about vision processing pipelines (an issue I know others have brought up) and the need for pure human driver skill will be less and less.\\u003c/p\\u003e\\n\\u003cp\\u003eIn conclusion, these are my personal thoughts on the matter and I’m very interested to see other opinions and history on it.\\u003c/p\\u003e\",\"post_number\":1,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-17T17:42:32.341Z\",\"reply_count\":1,\"reply_to_post_number\":null,\"quote_count\":0,\"incoming_link_count\":477,\"reads\":793,\"readers_count\":792,\"score\":2638.6,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"link_counts\":[{\"url\":\"https://www.tesla.com/autopilot\",\"internal\":false,\"reflection\":false,\"clicks\":6}],\"read\":true,\"user_title\":null,\"bookmarked\":false,\"actions_summary\":[{\"id\":2,\"count\":6}],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":54317,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"Main Programmer Dude | 8551\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/1\",\"custom_user_title\":\"Main Programmer Dude | 8551\",\"reactions\":[{\"id\":\"heart\",\"type\":\"emoji\",\"count\":6}],\"current_user_reaction\":null,\"reaction_users_count\":6,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null,\"can_vote\":false},{\"id\":3364399,\"name\":\"Eric (319)\",\"username\":\"EKM_319\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/ekm_319/{size}/230932_2.png\",\"created_at\":\"2024-04-17T18:03:30.209Z\",\"cooked\":\"\\u003cp\\u003eWhile I haven’t seen any real world applications of this in first. I agree with you that there are likely teams thinking about and considering this in upcoming years.\\u003c/p\\u003e\\n\\u003cp\\u003eI’d love to write a novel about ideas for this. But sadly being at work is my reality.\\u003c/p\\u003e\\n\\u003cp\\u003eI think you’ll see in the coming years some more efforts to embrace Lidar for obstacle avoidance. I thought I read in docs or posts that pathplanner’s pathfinding already has something in place to support this if you have sensors to pass in additional obstacles than what’s specified in the navgrid. I haven’t dug deep into other teams using this in the past like 88 in 2022(?) And whether IR was affected by polycarb field walls or varying lighting conditions between fields.\\u003c/p\\u003e\\n\\u003cp\\u003eTime of flight cameras and stereo depth might be cool for this too. But I forget if there are external windows in a venue whether or not the Sun would affect reliability/ resolution. ( i know ToF really didn’t work great outside when I played with them in the past, and can’t remember if the wavelengths of light i’m worried about go thru glass), and stereo depth to my knowledge is very taxing on processors. Especially now that intel RealSense product line has been phasing out.\\u003c/p\\u003e\\n\\u003cp\\u003eAnother avenue I was wondering if teams would use this year was google coral / neural networks for bumpers detection. I thought I saw a few of the open datasets had notes, red robots, and blue robots ( oh my!) But i’m unaware of the accuracy rates of that detection.  But i bet you could calculate the rough distance to a robot based on the x or y fov the target took up, and the avarage bumper width or height.\\u003c/p\\u003e\\n\\u003cp\\u003eHope that helps from somebody who’s also interested in the topic!\\u003c/p\\u003e\",\"post_number\":2,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-17T18:03:30.209Z\",\"reply_count\":0,\"reply_to_post_number\":null,\"quote_count\":0,\"incoming_link_count\":16,\"reads\":710,\"readers_count\":709,\"score\":267.0,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"Eric (319)\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"read\":true,\"user_title\":\"\",\"bookmarked\":false,\"actions_summary\":[{\"id\":2,\"count\":3}],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":23786,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"Awaiting our Robot Overlords since 2013\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/2\",\"custom_user_title\":\"Awaiting our Robot Overlords since 2013\",\"reactions\":[{\"id\":\"heart\",\"type\":\"emoji\",\"count\":3}],\"current_user_reaction\":null,\"reaction_users_count\":3,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3364408,\"name\":\"Peter Johnson\",\"username\":\"Peter_Johnson\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/peter_johnson/{size}/10417_2.png\",\"created_at\":\"2024-04-17T18:14:29.727Z\",\"cooked\":\"\\u003cp\\u003eTeam 900 is probably the furthest along.\\u003c/p\\u003e\\n\\u003caside class=\\\"quote quote-modified\\\" data-post=\\\"20\\\" data-topic=\\\"440094\\\"\\u003e\\n  \\u003cdiv class=\\\"title\\\"\\u003e\\n    \\u003cdiv class=\\\"quote-controls\\\"\\u003e\\u003c/div\\u003e\\n    \\u003cimg loading=\\\"lazy\\\" alt=\\\"\\\" width=\\\"24\\\" height=\\\"24\\\" src=\\\"https://www.chiefdelphi.com/user_avatar/www.chiefdelphi.com/marshall/48/167657_2.png\\\" class=\\\"avatar\\\"\\u003e\\n    \\u003ca href=\\\"https://www.chiefdelphi.com/t/the-zebracorns-behind-the-stripes-design-code-and-build-blog-2023-2024/440094/20\\\"\\u003eThe Zebracorns - Behind the Stripes - Design, Code, and Build Blog (2023/2024)\\u003c/a\\u003e \\u003ca class=\\\"badge-category__wrapper \\\" href=\\\"/c/first/open-alliance/89\\\"\\u003e\\u003cspan data-category-id=\\\"89\\\" style=\\\"--category-badge-color: #0088CC; --category-badge-text-color: #FFFFFF; --parent-category-badge-color: #F7941D;\\\" data-parent-category-id=\\\"7\\\" data-drop-close=\\\"true\\\" class=\\\"badge-category --has-parent\\\"\\u003e\\u003cspan class=\\\"badge-category__name\\\"\\u003eOpen Alliance\\u003c/span\\u003e\\u003c/span\\u003e\\u003c/a\\u003e\\n  \\u003c/div\\u003e\\n  \\u003cblockquote\\u003e\\n    I suggest looking at this thread: \\u003ca href=\\\"https://www.chiefdelphi.com/t/zebros-2023/442044\\\" class=\\\"inline-onebox\\\"\\u003eZebROS 2023 - Zebracorns Whitepaper\\u003c/a\\u003e and reading the linked paper.  If we’re ever enabled to do so, we’ll find things to use it for. \\nShort video demo though: \\n  \\u003ca href=\\\"https://www.youtube.com/watch?v=K_xUQiB2VJc\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\"\\u003e\\n    [Particle Filter Demo]\\n  \\u003c/a\\u003e\\n\\n\\nOf note, that code was running on an Orin NX and it worked just fine.  We had 100hz code loops and now have 250hz so the Orin NX has a lot of capacity left. \\nEDIT: It occurs to me that a folks won’t bother reading and won’t get it so here’s another video from the robot’s …\\n  \\u003c/blockquote\\u003e\\n\\u003c/aside\\u003e\\n\\n\\u003cp\\u003eEg their video using YOLOv8 clearly shows other robot recognition.\\u003c/p\\u003e\\u003cdiv class=\\\"youtube-onebox lazy-video-container\\\" data-video-id=\\\"p-PIHN8kKPE\\\" data-video-title=\\\"Initial results using YOLOv8 object detection model\\\" data-video-start-time=\\\"\\\" data-provider-name=\\\"youtube\\\"\\u003e\\n  \\u003ca href=\\\"https://www.youtube.com/watch?v=p-PIHN8kKPE\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\"\\u003e\\n    \\u003cimg class=\\\"youtube-thumbnail\\\" src=\\\"https://img.youtube.com/vi/p-PIHN8kKPE/maxresdefault.jpg\\\" title=\\\"Initial results using YOLOv8 object detection model\\\" width=\\\"690\\\" height=\\\"388\\\"\\u003e\\n  \\u003c/a\\u003e\\n\\u003c/div\\u003e\\n\",\"post_number\":3,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-17T18:16:37.610Z\",\"reply_count\":2,\"reply_to_post_number\":null,\"quote_count\":0,\"incoming_link_count\":17,\"reads\":617,\"readers_count\":616,\"score\":503.4,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"Peter Johnson\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"link_counts\":[{\"url\":\"https://www.youtube.com/watch?v=p-PIHN8kKPE\",\"internal\":false,\"reflection\":false,\"clicks\":12},{\"url\":\"https://www.chiefdelphi.com/t/the-zebracorns-behind-the-stripes-design-code-and-build-blog-2023-2024/440094/20\",\"internal\":true,\"reflection\":false,\"title\":\"The Zebracorns - Behind the Stripes - Design, Code, and Build Blog (2023/2024)\",\"clicks\":0}],\"read\":true,\"user_title\":\"\",\"bookmarked\":false,\"actions_summary\":[{\"id\":2,\"count\":19}],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":13575,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"Volunteer WPILib Developer, CSA\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/3\",\"custom_user_title\":\"Volunteer WPILib Developer, CSA\",\"reactions\":[{\"id\":\"heart\",\"type\":\"emoji\",\"count\":19},{\"id\":\"open_mouth\",\"type\":\"emoji\",\"count\":1}],\"current_user_reaction\":null,\"reaction_users_count\":20,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3364420,\"name\":\"Eric (319)\",\"username\":\"EKM_319\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/ekm_319/{size}/230932_2.png\",\"created_at\":\"2024-04-17T18:32:21.861Z\",\"cooked\":\"\\u003cp\\u003eHoly hand grenades Batman.\\u003c/p\\u003e\\n\\u003cp\\u003eI didn’t think anybody was running this level of detection at this point. That looks awesome\\u003c/p\\u003e\",\"post_number\":4,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-17T18:32:21.861Z\",\"reply_count\":0,\"reply_to_post_number\":3,\"quote_count\":0,\"incoming_link_count\":2,\"reads\":568,\"readers_count\":567,\"score\":138.6,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"Eric (319)\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"read\":true,\"user_title\":\"\",\"reply_to_user\":{\"id\":13575,\"username\":\"Peter_Johnson\",\"name\":\"Peter Johnson\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/peter_johnson/{size}/10417_2.png\"},\"bookmarked\":false,\"actions_summary\":[{\"id\":2,\"count\":1}],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":23786,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"Awaiting our Robot Overlords since 2013\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/4\",\"custom_user_title\":\"Awaiting our Robot Overlords since 2013\",\"reactions\":[{\"id\":\"heart\",\"type\":\"emoji\",\"count\":1}],\"current_user_reaction\":null,\"reaction_users_count\":1,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3364451,\"name\":\"Chris Gerth\",\"username\":\"gerthworm\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/gerthworm/{size}/268658_2.png\",\"created_at\":\"2024-04-17T19:28:27.544Z\",\"cooked\":\"\\u003cp\\u003eSo, to be clear, I think your thread title is one block in a system for a more-automated robot.\\u003c/p\\u003e\\n\\u003cp\\u003eI’ve laid this out more than once, but not recently.\\u003c/p\\u003e\\n\\u003cp\\u003eToday’s robots are already highly automated, especially in autonomous. They execute a fixed sequence of steps \\u003cem\\u003eassuming\\u003c/em\\u003e a fixed notion of where the driveable notions of the field are, with an assumption of gamepiece location and what the optimal scoring strategy is.\\u003c/p\\u003e\\n\\u003cp\\u003eBuilding blocks teams are currently assembling to achieve this:\\u003c/p\\u003e\\n\\u003col\\u003e\\n\\u003cli\\u003ecommand based architecture to create software-defined robot behaviors\\u003c/li\\u003e\\n\\u003cli\\u003eLocalization (including apriltags)\\u003c/li\\u003e\\n\\u003cli\\u003ePathplanning with fixed paths\\u003c/li\\u003e\\n\\u003c/ol\\u003e\\n\\u003cp\\u003eSome additional building blocks that could be developed:\\u003c/p\\u003e\\n\\u003col\\u003e\\n\\u003cli\\u003eVision-based gamepiece identification to find “target” locations\\u003c/li\\u003e\\n\\u003cli\\u003eVision-based robot identification to identify “keep-out” locations\\\"\\u003c/li\\u003e\\n\\u003cli\\u003eDynamic pathplanning with 1 and 2 in mind (PathPlanner’s already got some building blocks for this too)\\u003c/li\\u003e\\n\\u003cli\\u003eControllers which allow operators to indicate \\u003cem\\u003ewhat\\u003c/em\\u003e they want the robot to do (say, a queue of tasks), not \\u003cem\\u003ehow\\u003c/em\\u003e to do it (\\u003cem\\u003enot\\u003c/em\\u003e joysticks for motion commands).\\u003c/li\\u003e\\n\\u003c/ol\\u003e\\n\\u003cp\\u003eCritically, at this point, you’re still using human drivers to derive a task list. I would call this phase 1. “Task Automation”\\u003c/p\\u003e\\n\\u003cp\\u003eAn \\u003cem\\u003eadditional\\u003c/em\\u003e layer could be to also automate generating that task list, which would require blocks like:\\u003c/p\\u003e\\n\\u003col\\u003e\\n\\u003cli\\u003eAbility to read game state in from \\u003cspan class=\\\"abbreviation\\\"\\u003eFMS\\u003ctemplate class=\\\"tooltiptext\\\" data-text=\\\"Field Management System\\\"\\u003e\\u003c/template\\u003e\\u003c/span\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003eAbility to guess intent of other robots\\u003c/li\\u003e\\n\\u003cli\\u003eHand written (or AI trained??) algorithm for making decisions\\u003c/li\\u003e\\n\\u003cli\\u003e???\\u003c/li\\u003e\\n\\u003c/ol\\u003e\\n\\u003cp\\u003eWhatever those blocks are, they \\u003cem\\u003ethen and only then\\u003c/em\\u003e take the human’s place fully automating the decision making on \\u003cem\\u003ewhat\\u003c/em\\u003e the robot should do. Call that phase 2. “Mission Automation”.\\u003c/p\\u003e\\n\\u003cp\\u003eI think we’re just a few seasons out from seeing teams implement a phase 1 on the field and score well (IE, it’s a competitive advantage). Phase 2 is \\u003cem\\u003emuch\\u003c/em\\u003e longer term.\\u003c/p\\u003e\\n\\u003caside class=\\\"quote no-group\\\" data-username=\\\"SirFire\\\" data-post=\\\"1\\\" data-topic=\\\"463159\\\"\\u003e\\n\\u003cdiv class=\\\"title\\\"\\u003e\\n\\u003cdiv class=\\\"quote-controls\\\"\\u003e\\u003c/div\\u003e\\n\\u003cimg loading=\\\"lazy\\\" alt=\\\"\\\" width=\\\"24\\\" height=\\\"24\\\" src=\\\"https://www.chiefdelphi.com/user_avatar/www.chiefdelphi.com/sirfire/48/222590_2.png\\\" class=\\\"avatar\\\"\\u003e SirFire:\\u003c/div\\u003e\\n\\u003cblockquote\\u003e\\n\\u003cp\\u003eMy main concern is that, if such technology becomes more common and/or available for purchase, students will no longer need to learn about vision processing pipelines (an issue I know others have brought up)\\u003c/p\\u003e\\n\\u003c/blockquote\\u003e\\n\\u003c/aside\\u003e\\n\\u003cp\\u003eLike most engineering exercises, we’re just adding more layers. \\u003cspan class=\\\"abbreviation\\\"\\u003eFRC\\u003ctemplate class=\\\"tooltiptext\\\" data-text=\\\"FIRST Robotics Competition\\\"\\u003e\\u003c/template\\u003e\\u003c/span\\u003e in general is moving up in the technical stack over the years (remember 2008 when your code ran on bare metal on the CPU?). Today’s challenge is less and less “create X from scratch”, and more and more “integrate X into your robot”. Which… is still not easy - we’ve got 50 slides we’re about to present on it this week \\u003cimg src=\\\"https://www.chiefdelphi.com/images/emoji/google/slight_smile.png?v=12\\\" title=\\\":slight_smile:\\\" class=\\\"emoji\\\" alt=\\\":slight_smile:\\\" loading=\\\"lazy\\\" width=\\\"20\\\" height=\\\"20\\\"\\u003e .\\u003c/p\\u003e\\n\\u003caside class=\\\"quote no-group\\\" data-username=\\\"SirFire\\\" data-post=\\\"1\\\" data-topic=\\\"463159\\\"\\u003e\\n\\u003cdiv class=\\\"title\\\"\\u003e\\n\\u003cdiv class=\\\"quote-controls\\\"\\u003e\\u003c/div\\u003e\\n\\u003cimg loading=\\\"lazy\\\" alt=\\\"\\\" width=\\\"24\\\" height=\\\"24\\\" src=\\\"https://www.chiefdelphi.com/user_avatar/www.chiefdelphi.com/sirfire/48/222590_2.png\\\" class=\\\"avatar\\\"\\u003e SirFire:\\u003c/div\\u003e\\n\\u003cblockquote\\u003e\\n\\u003cp\\u003ethe need for pure human driver skill will be less and less.\\u003c/p\\u003e\\n\\u003c/blockquote\\u003e\\n\\u003c/aside\\u003e\\n\\u003cp\\u003eThis is perhaps more real as you move into my described autonomy. Still, this isn’t unique here. As humans make better and better tools, you move away from the need for vast numbers of people who can manually execute a task, and increase the need for people who can design tools to execute tasks automatically.\\u003c/p\\u003e\",\"post_number\":5,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-17T19:29:07.751Z\",\"reply_count\":1,\"reply_to_post_number\":null,\"quote_count\":1,\"incoming_link_count\":7,\"reads\":526,\"readers_count\":525,\"score\":310.2,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"Chris Gerth\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"read\":true,\"user_title\":\"\",\"bookmarked\":false,\"actions_summary\":[{\"id\":2,\"count\":12}],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":24617,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/5\",\"custom_user_title\":null,\"reactions\":[{\"id\":\"heart\",\"type\":\"emoji\",\"count\":11},{\"id\":\"+1\",\"type\":\"emoji\",\"count\":1}],\"current_user_reaction\":null,\"reaction_users_count\":12,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3364461,\"name\":\"Levi\",\"username\":\"nobody5690\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/nobody5690/{size}/203585_2.png\",\"created_at\":\"2024-04-17T19:39:10.000Z\",\"cooked\":\"\\u003cp\\u003eThis is a very good rundown of the steps involved. Having devoted a significant amount of time towards task automation this year (we reached a level where we can be extremely confident in auto scoring at our home field, but not confident enough to run it in a match) I would say the building blocks are definitely already in place to implement task automation, I would expect to see teams running it on field next year. That said, implementing it in a way where it performs better than a human driver at the same tasks is a whole order of magnitude higher.\\u003c/p\\u003e\",\"post_number\":6,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-17T19:47:40.716Z\",\"reply_count\":0,\"reply_to_post_number\":5,\"quote_count\":0,\"incoming_link_count\":1,\"reads\":429,\"readers_count\":428,\"score\":105.8,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"Levi\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":2,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"read\":true,\"user_title\":\"\",\"reply_to_user\":{\"id\":24617,\"username\":\"gerthworm\",\"name\":\"Chris Gerth\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/gerthworm/{size}/268658_2.png\"},\"bookmarked\":false,\"actions_summary\":[{\"id\":2,\"count\":1}],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":49892,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"5690AM | https://lumynlabs.com\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/6\",\"custom_user_title\":\"5690AM | https://lumynlabs.com\",\"reactions\":[{\"id\":\"heart\",\"type\":\"emoji\",\"count\":1}],\"current_user_reaction\":null,\"reaction_users_count\":1,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3364467,\"name\":\"joemost\",\"username\":\"joemost\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/joemost/{size}/11114_2.png\",\"created_at\":\"2024-04-17T19:46:42.081Z\",\"cooked\":\"\\u003cp\\u003ei’ve seen this video a few times now and it remains insanely cool.\\u003c/p\\u003e\\n\\u003cp\\u003eWonder if they used any of this this year, mid line auto would have a ton of relevancy with this\\u003c/p\\u003e\",\"post_number\":7,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-17T19:46:42.081Z\",\"reply_count\":1,\"reply_to_post_number\":3,\"quote_count\":0,\"incoming_link_count\":0,\"reads\":423,\"readers_count\":422,\"score\":89.6,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"joemost\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"read\":true,\"user_title\":\"\",\"reply_to_user\":{\"id\":13575,\"username\":\"Peter_Johnson\",\"name\":\"Peter Johnson\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/peter_johnson/{size}/10417_2.png\"},\"bookmarked\":false,\"actions_summary\":[],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":20003,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/7\",\"custom_user_title\":null,\"reactions\":[],\"current_user_reaction\":null,\"reaction_users_count\":0,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3364487,\"name\":\"Roman A.\",\"username\":\"RomanTechPlus\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/romantechplus/{size}/214929_2.png\",\"created_at\":\"2024-04-17T20:11:13.215Z\",\"cooked\":\"\\u003cp\\u003eA great way to get started in 3d object tracking is with a depth camera like the oak-D. So far weve had decent success with robot and note tracking on the device and with a relatively small dev time allocation. The hardware takes some gettig used to since its a bit limited and models need to be converted, bit its incredibly simple to use. It does come with the \\u003cspan class=\\\"abbreviation\\\"\\u003eFRC\\u003ctemplate class=\\\"tooltiptext\\\" data-text=\\\"FIRST Robotics Competition\\\"\\u003e\\u003c/template\\u003e\\u003c/span\\u003e early addoper tax of having to worite all the robot to coproc intefsce code yourself, but thats mostly just messing with NT4.\\u003c/p\\u003e\\n\\u003cp\\u003eFor the record: the dpeth error weve observed is in line with their published specs (6% at 8m for the OAK-D lite) tho latency can def be an issue with us getting about 300ms (EDIT: 200ms after some tuning today) with on device processing on a Yolov5n modle. Tho we are looking into reducing this by using yolov6n or moving the ml on coproc.\\u003c/p\\u003e\",\"post_number\":8,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-18T02:53:37.115Z\",\"reply_count\":2,\"reply_to_post_number\":null,\"quote_count\":0,\"incoming_link_count\":2,\"reads\":413,\"readers_count\":412,\"score\":132.6,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"Roman A.\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":3,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"read\":true,\"user_title\":null,\"bookmarked\":false,\"actions_summary\":[{\"id\":2,\"count\":2}],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":53160,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"5104 BreakerBots | Software and Additive Mfg. Lead\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/8\",\"custom_user_title\":\"5104 BreakerBots | Software and Additive Mfg. Lead\",\"reactions\":[{\"id\":\"heart\",\"type\":\"emoji\",\"count\":2}],\"current_user_reaction\":null,\"reaction_users_count\":2,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3364929,\"name\":\"Ben Bernard (He/Him)\",\"username\":\"BenBernard\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/benbernard/{size}/8721_2.png\",\"created_at\":\"2024-04-18T12:15:00.125Z\",\"cooked\":\"\\u003cp\\u003eSeveral teams now attempt to use vision to detect if another bot has already grabbed a center-line note and switch to a different note.  Next step is to use bumper detection to know if another bot is \\u003cem\\u003elikely\\u003c/em\\u003e to beat you to a note and switch notes \\u003cem\\u003ebefore\\u003c/em\\u003e that happens.\\u003c/p\\u003e\",\"post_number\":9,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-18T12:15:00.125Z\",\"reply_count\":0,\"reply_to_post_number\":7,\"quote_count\":0,\"incoming_link_count\":0,\"reads\":357,\"readers_count\":356,\"score\":71.4,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"Ben Bernard (He/Him)\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"read\":true,\"user_title\":\"\",\"reply_to_user\":{\"id\":20003,\"username\":\"joemost\",\"name\":\"joemost\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/joemost/{size}/11114_2.png\"},\"bookmarked\":false,\"actions_summary\":[],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":26824,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"Controls Mentor (semi-retired) \"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/9\",\"custom_user_title\":\"Controls Mentor (semi-retired) \",\"reactions\":[],\"current_user_reaction\":null,\"reaction_users_count\":0,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3365496,\"name\":\"Roman A.\",\"username\":\"RomanTechPlus\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/romantechplus/{size}/214929_2.png\",\"created_at\":\"2024-04-19T01:41:57.894Z\",\"cooked\":\"\\u003cp\\u003eHeres what our 3D object detection looks like.\\u003cbr\\u003e\\n(The notes also have 3d data, its just very small on the screen).\\u003c/p\\u003e\\n\\u003cp\\u003e\\u003cdiv class=\\\"lightbox-wrapper\\\"\\u003e\\u003ca class=\\\"lightbox\\\" href=\\\"https://www.chiefdelphi.com/uploads/default/original/3X/0/4/04afe2e1861d866a3a2258c280f38b8dd4d7e914.jpeg\\\" data-download-href=\\\"https://www.chiefdelphi.com/uploads/default/04afe2e1861d866a3a2258c280f38b8dd4d7e914\\\" title=\\\"20240418_174458\\\"\\u003e\\u003cimg src=\\\"https://www.chiefdelphi.com/uploads/default/optimized/3X/0/4/04afe2e1861d866a3a2258c280f38b8dd4d7e914_2_690x359.jpeg\\\" alt=\\\"20240418_174458\\\" data-base62-sha1=\\\"FsK9DtNQTkZVf2XlPk51spbiOU\\\" width=\\\"690\\\" height=\\\"359\\\" srcset=\\\"https://www.chiefdelphi.com/uploads/default/optimized/3X/0/4/04afe2e1861d866a3a2258c280f38b8dd4d7e914_2_690x359.jpeg, https://www.chiefdelphi.com/uploads/default/optimized/3X/0/4/04afe2e1861d866a3a2258c280f38b8dd4d7e914_2_1035x538.jpeg 1.5x, https://www.chiefdelphi.com/uploads/default/optimized/3X/0/4/04afe2e1861d866a3a2258c280f38b8dd4d7e914_2_1380x718.jpeg 2x\\\" data-dominant-color=\\\"868E8E\\\"\\u003e\\u003cdiv class=\\\"meta\\\"\\u003e\\u003csvg class=\\\"fa d-icon d-icon-far-image svg-icon\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cuse href=\\\"#far-image\\\"\\u003e\\u003c/use\\u003e\\u003c/svg\\u003e\\u003cspan class=\\\"filename\\\"\\u003e20240418_174458\\u003c/span\\u003e\\u003cspan class=\\\"informations\\\"\\u003e1920×999 166 KB\\u003c/span\\u003e\\u003csvg class=\\\"fa d-icon d-icon-discourse-expand svg-icon\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cuse href=\\\"#discourse-expand\\\"\\u003e\\u003c/use\\u003e\\u003c/svg\\u003e\\u003c/div\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\u003c/p\\u003e\",\"post_number\":11,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-19T01:41:57.894Z\",\"reply_count\":1,\"reply_to_post_number\":8,\"quote_count\":0,\"incoming_link_count\":2,\"reads\":337,\"readers_count\":336,\"score\":112.4,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"Roman A.\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"link_counts\":[{\"url\":\"https://www.chiefdelphi.com/uploads/default/original/3X/0/4/04afe2e1861d866a3a2258c280f38b8dd4d7e914.jpeg\",\"internal\":true,\"reflection\":false,\"clicks\":0}],\"read\":true,\"user_title\":null,\"reply_to_user\":{\"id\":53160,\"username\":\"RomanTechPlus\",\"name\":\"Roman A.\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/romantechplus/{size}/214929_2.png\"},\"bookmarked\":false,\"actions_summary\":[{\"id\":2,\"count\":4}],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":53160,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"5104 BreakerBots | Software and Additive Mfg. Lead\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/11\",\"custom_user_title\":\"5104 BreakerBots | Software and Additive Mfg. Lead\",\"reactions\":[{\"id\":\"+1\",\"type\":\"emoji\",\"count\":2},{\"id\":\"heart\",\"type\":\"emoji\",\"count\":2}],\"current_user_reaction\":null,\"reaction_users_count\":4,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3365523,\"name\":\"Andrew Schreiber\",\"username\":\"Andrew_Schreiber\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/andrew_schreiber/{size}/209563_2.png\",\"created_at\":\"2024-04-19T02:43:37.897Z\",\"cooked\":\"\\u003cp\\u003eSo let’s talk about how FSD works under the hood.\\u003c/p\\u003e\\n\\u003cp\\u003eSensor wise, Elon has a fascination with vision only. And Teslas do not possess depth cameras. Under the hood they are using monocular depth estimation models that have been trained on outdoor environments to estimate metric depth of each pixel. From there they put their objects into a “4d vector space” generated off a custom built model. This is a fancy way of saying “we put it in a representation that we can ask questions about objects as a function of location and time”. This allows them to reason about their operating space.\\u003c/p\\u003e\\n\\u003cp\\u003eSo if you wanted to replicate this in \\u003cspan class=\\\"abbreviation\\\"\\u003eFRC\\u003ctemplate class=\\\"tooltiptext\\\" data-text=\\\"FIRST Robotics Competition\\\"\\u003e\\u003c/template\\u003e\\u003c/span\\u003e you have one big choice early on - depth camera or monocular camera?  Depth cameras are more expensive and require a bit more compute (unless you’re using something like the Intel Realsense line which does processing onboard, though I know 900 has gone away from these) Monocular cameras are cheaper but give require more thought on how to get depth out of it.\\u003c/p\\u003e\\n\\u003cp\\u003eAfter that you need to build some way of tracking objects between frames. Tesla uses their ML expertise here and their model takes multiple video frames (and in fact multiple camera angles) into account here. I don’t recommend going down this route unless you have several ML experts in house, and a lot of compute. I’ve found \\u003ca href=\\\"https://www.youtube.com/watch?v=RWRJYuvI6M4\\\" rel=\\\"noopener nofollow ugc\\\"\\u003efairly adequate behavior\\u003c/a\\u003e over short durations (5-10 seconds) using simple tools like \\u003ca href=\\\"https://pypi.org/project/bytetracker/\\\" rel=\\\"noopener nofollow ugc\\\"\\u003eByteTrack\\u003c/a\\u003e . My preferred approach here is a \\u003cem\\u003every\\u003c/em\\u003e simple ECS system to allow me to simulate the physics of found dynamic objects, persist static objects, and decay detections over time. It’s not as elegant as the ML solution likely but I’ve used ECS systems for a lot in the past and know I can make them work.\\u003c/p\\u003e\\n\\u003cp\\u003eAfter that you \\u003cem\\u003ejust\\u003c/em\\u003e need to build an API around the simulation system to be able to ask temporal/spatial questions about it. This drove most of why I went with the ECS approach as they are effective at being able to move forward and backward in time.\\u003c/p\\u003e\\n\\u003chr\\u003e\\n\\u003cp\\u003ePersonally, I’ve gone down the monocular approach. I have detection and relative depth working ok, need to find some time to sit down and read the \\u003ca href=\\\"https://github.com/isl-org/ZoeDepth\\\" rel=\\\"noopener nofollow ugc\\\"\\u003epapers on ZoeDepth’s metric depth estimation\\u003c/a\\u003e to see if I can get reliable information. Fortunately I have a virtual environment that I can take a robot path, put a camera on the robot, and output metric depth training data if I need to fine tune (\\u003cspan class=\\\"abbreviation\\\"\\u003eFRC\\u003ctemplate class=\\\"tooltiptext\\\" data-text=\\\"FIRST Robotics Competition\\\"\\u003e\\u003c/template\\u003e\\u003c/span\\u003e fields are fairly nice in that they are constrained environments making this much simpler…in theory).\\u003cbr\\u003e\\nVirtual:\\u003cbr\\u003e\\n\\u003cdiv class=\\\"lightbox-wrapper\\\"\\u003e\\u003ca class=\\\"lightbox\\\" href=\\\"https://www.chiefdelphi.com/uploads/default/original/3X/0/e/0e4f3bbee36f8fb78641380578e194637d3c31d7.jpeg\\\" data-download-href=\\\"https://www.chiefdelphi.com/uploads/default/0e4f3bbee36f8fb78641380578e194637d3c31d7\\\" title=\\\"000043\\\"\\u003e\\u003cimg src=\\\"https://www.chiefdelphi.com/uploads/default/optimized/3X/0/e/0e4f3bbee36f8fb78641380578e194637d3c31d7_2_250x250.jpeg\\\" alt=\\\"000043\\\" data-base62-sha1=\\\"22ArmrFJD4TMwEs1ORNwII0mXWf\\\" width=\\\"250\\\" height=\\\"250\\\" srcset=\\\"https://www.chiefdelphi.com/uploads/default/optimized/3X/0/e/0e4f3bbee36f8fb78641380578e194637d3c31d7_2_250x250.jpeg, https://www.chiefdelphi.com/uploads/default/optimized/3X/0/e/0e4f3bbee36f8fb78641380578e194637d3c31d7_2_375x375.jpeg 1.5x, https://www.chiefdelphi.com/uploads/default/optimized/3X/0/e/0e4f3bbee36f8fb78641380578e194637d3c31d7_2_500x500.jpeg 2x\\\" data-dominant-color=\\\"3B3838\\\"\\u003e\\u003cdiv class=\\\"meta\\\"\\u003e\\u003csvg class=\\\"fa d-icon d-icon-far-image svg-icon\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cuse href=\\\"#far-image\\\"\\u003e\\u003c/use\\u003e\\u003c/svg\\u003e\\u003cspan class=\\\"filename\\\"\\u003e000043\\u003c/span\\u003e\\u003cspan class=\\\"informations\\\"\\u003e640×640 41.1 KB\\u003c/span\\u003e\\u003csvg class=\\\"fa d-icon d-icon-discourse-expand svg-icon\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cuse href=\\\"#discourse-expand\\\"\\u003e\\u003c/use\\u003e\\u003c/svg\\u003e\\u003c/div\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\u003cbr\\u003e\\nRel Depth + Note Detector\\u003cbr\\u003e\\n\\u003cdiv class=\\\"lightbox-wrapper\\\"\\u003e\\u003ca class=\\\"lightbox\\\" href=\\\"https://www.chiefdelphi.com/uploads/default/original/3X/8/4/844b0b9a828b306b7735b1bd10e901331fe1b3a0.png\\\" data-download-href=\\\"https://www.chiefdelphi.com/uploads/default/844b0b9a828b306b7735b1bd10e901331fe1b3a0\\\" title=\\\"PNG image\\\"\\u003e\\u003cimg src=\\\"https://www.chiefdelphi.com/uploads/default/optimized/3X/8/4/844b0b9a828b306b7735b1bd10e901331fe1b3a0_2_248x250.png\\\" alt=\\\"PNG image\\\" data-base62-sha1=\\\"iSjOdwpSdjdGDuRPVAw7VJm0yre\\\" width=\\\"248\\\" height=\\\"250\\\" srcset=\\\"https://www.chiefdelphi.com/uploads/default/optimized/3X/8/4/844b0b9a828b306b7735b1bd10e901331fe1b3a0_2_248x250.png, https://www.chiefdelphi.com/uploads/default/optimized/3X/8/4/844b0b9a828b306b7735b1bd10e901331fe1b3a0_2_372x375.png 1.5x, https://www.chiefdelphi.com/uploads/default/optimized/3X/8/4/844b0b9a828b306b7735b1bd10e901331fe1b3a0_2_496x500.png 2x\\\" data-dominant-color=\\\"414141\\\"\\u003e\\u003cdiv class=\\\"meta\\\"\\u003e\\u003csvg class=\\\"fa d-icon d-icon-far-image svg-icon\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cuse href=\\\"#far-image\\\"\\u003e\\u003c/use\\u003e\\u003c/svg\\u003e\\u003cspan class=\\\"filename\\\"\\u003ePNG image\\u003c/span\\u003e\\u003cspan class=\\\"informations\\\"\\u003e1272×1278 52.4 KB\\u003c/span\\u003e\\u003csvg class=\\\"fa d-icon d-icon-discourse-expand svg-icon\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cuse href=\\\"#discourse-expand\\\"\\u003e\\u003c/use\\u003e\\u003c/svg\\u003e\\u003c/div\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\u003c/p\\u003e\\n\\u003chr\\u003e\\n\\u003cp\\u003eThis is all a fun theory but I’m not entirely sure how useful it will be in \\u003cspan class=\\\"abbreviation\\\"\\u003eFRC\\u003ctemplate class=\\\"tooltiptext\\\" data-text=\\\"FIRST Robotics Competition\\\"\\u003e\\u003c/template\\u003e\\u003c/span\\u003e in the short term. Our required precisions are fairly high. If you look closely at the data coming out of FSD it is regularly off by 5-10\\\" This is \\u003cem\\u003efine\\u003c/em\\u003e when you have a 7’ car and a nominally 12’ lane and need to stop a reasonable distance from other cars. (My old Tesla used to scream coming into the garage yelling I needed to stop when I was a foot away from the side of the garage) but in \\u003cspan class=\\\"abbreviation\\\"\\u003eFRC\\u003ctemplate class=\\\"tooltiptext\\\" data-text=\\\"FIRST Robotics Competition\\\"\\u003e\\u003c/template\\u003e\\u003c/span\\u003e those distances are worse than what tuned wheel odometry can get you. Yes tools like particle filters can help with localization \\u003cem\\u003ebut\\u003c/em\\u003e those have additional compute requirements too (though likely lighter than the approach outlined above).\\u003c/p\\u003e\\n\\u003cp\\u003eAnd this has been my brain dump on FSD for the night, hope some folks found it mildly interesting.\\u003c/p\\u003e\",\"post_number\":12,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-19T02:43:37.897Z\",\"reply_count\":2,\"reply_to_post_number\":null,\"quote_count\":0,\"incoming_link_count\":6,\"reads\":332,\"readers_count\":331,\"score\":226.4,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"Andrew Schreiber\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"link_counts\":[{\"url\":\"https://github.com/isl-org/ZoeDepth\",\"internal\":false,\"reflection\":false,\"title\":\"GitHub - isl-org/ZoeDepth: Metric depth estimation from a single image\",\"clicks\":8},{\"url\":\"https://www.youtube.com/watch?v=RWRJYuvI6M4\",\"internal\":false,\"reflection\":false,\"clicks\":4},{\"url\":\"https://pypi.org/project/bytetracker/\",\"internal\":false,\"reflection\":false,\"title\":\"bytetracker · PyPI\",\"clicks\":2},{\"url\":\"https://www.chiefdelphi.com/uploads/default/original/3X/8/4/844b0b9a828b306b7735b1bd10e901331fe1b3a0.png\",\"internal\":true,\"reflection\":false,\"clicks\":0},{\"url\":\"https://www.chiefdelphi.com/uploads/default/original/3X/0/e/0e4f3bbee36f8fb78641380578e194637d3c31d7.jpeg\",\"internal\":true,\"reflection\":false,\"clicks\":0}],\"read\":true,\"user_title\":\"\",\"bookmarked\":false,\"actions_summary\":[{\"id\":2,\"count\":11}],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":4297,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"I'm Retired\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/12\",\"custom_user_title\":\"I'm Retired\",\"reactions\":[{\"id\":\"heart\",\"type\":\"emoji\",\"count\":8},{\"id\":\"+1\",\"type\":\"emoji\",\"count\":3}],\"current_user_reaction\":null,\"reaction_users_count\":11,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3365528,\"name\":\"Roman A.\",\"username\":\"RomanTechPlus\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/romantechplus/{size}/214929_2.png\",\"created_at\":\"2024-04-19T03:01:21.468Z\",\"cooked\":\"\\u003cp\\u003eQuick note: The OAK-D has a very simple to set up inter-frame tracking solution for their depth cameras\\u003c/p\\u003e\",\"post_number\":13,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-19T03:01:44.395Z\",\"reply_count\":1,\"reply_to_post_number\":12,\"quote_count\":0,\"incoming_link_count\":1,\"reads\":290,\"readers_count\":289,\"score\":68.0,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"Roman A.\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"read\":true,\"user_title\":null,\"reply_to_user\":{\"id\":4297,\"username\":\"Andrew_Schreiber\",\"name\":\"Andrew Schreiber\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/andrew_schreiber/{size}/209563_2.png\"},\"bookmarked\":false,\"actions_summary\":[],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":53160,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"5104 BreakerBots | Software and Additive Mfg. Lead\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/13\",\"custom_user_title\":\"5104 BreakerBots | Software and Additive Mfg. Lead\",\"reactions\":[],\"current_user_reaction\":null,\"reaction_users_count\":0,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3365534,\"name\":\"\",\"username\":\"SirFire\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/sirfire/{size}/222590_2.png\",\"created_at\":\"2024-04-19T03:13:54.948Z\",\"cooked\":\"\\u003cp\\u003eThat’s super cool! If I may ask, how accurate have you found that to be and how long have you been using it? What hardware are you using to accomplish it? I’m curious about what can be done with just a Raspberry Pi 4 and a decent camera as far as object detection goes.\\u003c/p\\u003e\",\"post_number\":14,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-19T03:13:54.948Z\",\"reply_count\":1,\"reply_to_post_number\":11,\"quote_count\":0,\"incoming_link_count\":5,\"reads\":289,\"readers_count\":288,\"score\":87.8,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"read\":true,\"user_title\":null,\"reply_to_user\":{\"id\":53160,\"username\":\"RomanTechPlus\",\"name\":\"Roman A.\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/romantechplus/{size}/214929_2.png\"},\"bookmarked\":false,\"actions_summary\":[],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":54317,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"Main Programmer Dude | 8551\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/14\",\"custom_user_title\":\"Main Programmer Dude | 8551\",\"reactions\":[],\"current_user_reaction\":null,\"reaction_users_count\":0,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3365540,\"name\":\"Roman A.\",\"username\":\"RomanTechPlus\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/romantechplus/{size}/214929_2.png\",\"created_at\":\"2024-04-19T03:22:22.039Z\",\"cooked\":\"\\u003cp\\u003eWe are using an OAK-D lite stereo depth camera (looking to switch to the OAK-D S2 or one of their upcoming RVC4 products soon tho) id say the accuracy is relatively in line tieht their published specs if somehat worse, but sill def below 10% \\u003cspan class=\\\"mention\\\"\\u003e@6-8M\\u003c/span\\u003e with their lowest res stereo pair camera (480p on the lite). This is a relatively new systen for us and we only started development on monday lmao, tho ive pretty much pulled back to back all nighters getting it to this point (it can interface with robot code). As far simple object detection, the rpi4 does not have a TPU so you won’t see  good real-time performance, id look into 2d object tracking with photonvision and a orange pi 5, weve been running a similar systen to that since 2023 offseason.\\u003c/p\\u003e\",\"post_number\":15,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-19T03:24:54.210Z\",\"reply_count\":0,\"reply_to_post_number\":14,\"quote_count\":0,\"incoming_link_count\":0,\"reads\":288,\"readers_count\":287,\"score\":57.6,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"Roman A.\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"read\":true,\"user_title\":null,\"reply_to_user\":{\"id\":54317,\"username\":\"SirFire\",\"name\":\"\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/sirfire/{size}/222590_2.png\"},\"bookmarked\":false,\"actions_summary\":[],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":53160,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"5104 BreakerBots | Software and Additive Mfg. Lead\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/15\",\"custom_user_title\":\"5104 BreakerBots | Software and Additive Mfg. Lead\",\"reactions\":[],\"current_user_reaction\":null,\"reaction_users_count\":0,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3365568,\"name\":\"jdao\",\"username\":\"jdao\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/jdao/{size}/160969_2.png\",\"created_at\":\"2024-04-19T04:44:54.847Z\",\"cooked\":\"\\u003cp\\u003e200 ms seems fairly high. What resolution are you running the YOLO model at? Additionally, are you adjusting the model conversion settings/initializing the pipeline settings properly? Running the model on the camera should be doable with a reasonable latency value, even on a Raspberry Pi.\\u003c/p\\u003e\\n\\u003cp\\u003eFor reference, the performance specs of the OAK cameras running NN models can be found here for RVC2: \\u003ca href=\\\"https://docs.luxonis.com/projects/hardware/en/latest/pages/rvc/rvc2/#rvc2-nn-performance\\\" class=\\\"inline-onebox\\\"\\u003eRobotics Vision Core 2 (RVC2) — DepthAI Hardware Documentation 1.0.0 documentation\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp\\u003eFor RVC1, I was able to get ~40 FPS a few years ago using a YOLOv3 model on a Raspberry Pi 4. I can’t find latency values, but I imagine it would be much lower at this FPS.\\u003c/p\\u003e\",\"post_number\":16,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-19T04:44:54.847Z\",\"reply_count\":1,\"reply_to_post_number\":8,\"quote_count\":0,\"incoming_link_count\":1,\"reads\":280,\"readers_count\":279,\"score\":66.0,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"jdao\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"link_counts\":[{\"url\":\"https://docs.luxonis.com/projects/hardware/en/latest/pages/rvc/rvc2/#rvc2-nn-performance\",\"internal\":false,\"reflection\":false,\"title\":\"Robotics Vision Core 2 (RVC2) — DepthAI Hardware Documentation 1.0.0 documentation\",\"clicks\":3}],\"read\":true,\"user_title\":\"\",\"reply_to_user\":{\"id\":53160,\"username\":\"RomanTechPlus\",\"name\":\"Roman A.\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/romantechplus/{size}/214929_2.png\"},\"bookmarked\":false,\"actions_summary\":[],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":24178,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"4201 Mentor, CSA\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/16\",\"custom_user_title\":\"4201 Mentor, CSA\",\"reactions\":[],\"current_user_reaction\":null,\"reaction_users_count\":0,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3365578,\"name\":\"Roman A.\",\"username\":\"RomanTechPlus\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/romantechplus/{size}/214929_2.png\",\"created_at\":\"2024-04-19T05:34:31.686Z\",\"cooked\":\"\\u003cp\\u003eYOLOv8n at 640 x 640, i am well aware that this is an extremely heavy load and im currently exploring my options with different models. 640 x 640 was just what my dataset was and i wanted to get a working example out as quickly as possible. I was getting 500ms at 20fps, but changing some settings yealded lower latency but lower fps. Then again, i was having these issues even with yolov6 so idk.\\u003c/p\\u003e\",\"post_number\":17,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-19T06:30:04.958Z\",\"reply_count\":1,\"reply_to_post_number\":16,\"quote_count\":0,\"incoming_link_count\":3,\"reads\":263,\"readers_count\":262,\"score\":72.6,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"Roman A.\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":3,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"read\":true,\"user_title\":null,\"reply_to_user\":{\"id\":24178,\"username\":\"jdao\",\"name\":\"jdao\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/jdao/{size}/160969_2.png\"},\"bookmarked\":false,\"actions_summary\":[],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":53160,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"5104 BreakerBots | Software and Additive Mfg. Lead\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/17\",\"custom_user_title\":\"5104 BreakerBots | Software and Additive Mfg. Lead\",\"reactions\":[],\"current_user_reaction\":null,\"reaction_users_count\":0,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3365858,\"name\":\"Andrew Schreiber\",\"username\":\"Andrew_Schreiber\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/andrew_schreiber/{size}/209563_2.png\",\"created_at\":\"2024-04-19T18:57:51.738Z\",\"cooked\":\"\\u003cp\\u003eI assume you are referring to \\u003ca href=\\\"https://docs.luxonis.com/projects/api/en/latest/components/nodes/object_tracker/\\\" class=\\\"inline-onebox\\\" rel=\\\"noopener nofollow ugc\\\"\\u003eObjectTracker — DepthAI documentation | Luxonis\\u003c/a\\u003e?\\u003c/p\\u003e\\n\\u003cp\\u003eI do have an OAK-D sitting around, I’ve been considering it for my purposes but I’m of the opinion that multiple views is more important than precise depth numbers and the cost of multiple (~4-6) OAK-D is more than I’m willing to personally fund especially with the additional need of a more powerful coprocessor on top of it.\\u003c/p\\u003e\\n\\u003cp\\u003eIt’s also entirely possible I’m wrong, which is part of why I’m spending a lot of time building it out on the virtual field first. It’s pretty cheap to do.\\u003c/p\\u003e\",\"post_number\":18,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-19T18:57:51.738Z\",\"reply_count\":1,\"reply_to_post_number\":13,\"quote_count\":0,\"incoming_link_count\":0,\"reads\":249,\"readers_count\":248,\"score\":54.8,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"Andrew Schreiber\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"link_counts\":[{\"url\":\"https://docs.luxonis.com/projects/api/en/latest/components/nodes/object_tracker/\",\"internal\":false,\"reflection\":false,\"title\":\"ObjectTracker — DepthAI documentation | Luxonis\",\"clicks\":5}],\"read\":true,\"user_title\":\"\",\"reply_to_user\":{\"id\":53160,\"username\":\"RomanTechPlus\",\"name\":\"Roman A.\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/romantechplus/{size}/214929_2.png\"},\"bookmarked\":false,\"actions_summary\":[],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":4297,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"I'm Retired\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/18\",\"custom_user_title\":\"I'm Retired\",\"reactions\":[],\"current_user_reaction\":null,\"reaction_users_count\":0,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3365859,\"name\":\"Roman A.\",\"username\":\"RomanTechPlus\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/romantechplus/{size}/214929_2.png\",\"created_at\":\"2024-04-19T19:05:23.306Z\",\"cooked\":\"\\u003cp\\u003eI am indeed referring to the objectTracker feature. Precise dpeth opes a lot of feature possibilities that woud otherwise be impossible with simple 2d tracking, and thats why were pursuing it. That said, fir a low cost solution the OAK-D lite is quite good. My current plan for wide coveange is to use an OAK-D S2 for the robot’s “front” to get accurate depth estimates, and one or two OAK-D W’s for wide angel coverage.\\u003c/p\\u003e\",\"post_number\":19,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-19T19:05:23.306Z\",\"reply_count\":0,\"reply_to_post_number\":18,\"quote_count\":0,\"incoming_link_count\":0,\"reads\":238,\"readers_count\":237,\"score\":62.6,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"Roman A.\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"read\":true,\"user_title\":null,\"reply_to_user\":{\"id\":4297,\"username\":\"Andrew_Schreiber\",\"name\":\"Andrew Schreiber\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/andrew_schreiber/{size}/209563_2.png\"},\"bookmarked\":false,\"actions_summary\":[{\"id\":2,\"count\":1}],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":53160,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"5104 BreakerBots | Software and Additive Mfg. Lead\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/19\",\"custom_user_title\":\"5104 BreakerBots | Software and Additive Mfg. Lead\",\"reactions\":[{\"id\":\"heart\",\"type\":\"emoji\",\"count\":1}],\"current_user_reaction\":null,\"reaction_users_count\":1,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3368711,\"name\":\"jdao\",\"username\":\"jdao\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/jdao/{size}/160969_2.png\",\"created_at\":\"2024-04-22T00:30:56.533Z\",\"cooked\":\"\\u003cp\\u003eI’ve been looking at your code/the DepthAI examples and I think this is a bug with the DepthAI SDK. Specifically, using \\u003ccode\\u003eoak.callback()\\u003c/code\\u003e seems to have a significant impact on performance (about a ~50-75% FPS reduction!). You can get around this by modifying your custom python function and calling it using the \\u003ccode\\u003edecode_fn\\u003c/code\\u003e argument in \\u003ccode\\u003eoak.create_nn()\\u003c/code\\u003e, which seems to get around the issue (Documentation example \\u003ca href=\\\"https://docs.luxonis.com/projects/sdk/en/latest/samples/NNComponent/sdk_custom_decode/\\\"\\u003ehere\\u003c/a\\u003e).\\u003c/p\\u003e\\n\\u003cp\\u003eAll my prior DepthAI code used the Python API directly, as the SDK was just starting to be developed at the time. This didn’t run into as many performance issues, but looking back at it now, it is extremely messy/hard to follow.\\u003c/p\\u003e\",\"post_number\":20,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-22T00:30:56.533Z\",\"reply_count\":2,\"reply_to_post_number\":17,\"quote_count\":0,\"incoming_link_count\":0,\"reads\":225,\"readers_count\":224,\"score\":55.0,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"jdao\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"link_counts\":[{\"url\":\"https://docs.luxonis.com/projects/sdk/en/latest/samples/NNComponent/sdk_custom_decode/\",\"internal\":false,\"reflection\":false,\"title\":\"Custom Decode Function — DepthAI SDK Docs 1.13.1 documentation\",\"clicks\":2}],\"read\":true,\"user_title\":\"\",\"reply_to_user\":{\"id\":53160,\"username\":\"RomanTechPlus\",\"name\":\"Roman A.\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/romantechplus/{size}/214929_2.png\"},\"bookmarked\":false,\"actions_summary\":[],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":24178,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"4201 Mentor, CSA\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/20\",\"custom_user_title\":\"4201 Mentor, CSA\",\"reactions\":[],\"current_user_reaction\":null,\"reaction_users_count\":0,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null},{\"id\":3368721,\"name\":\"Rocky\",\"username\":\"Rocky_S\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/rocky_s/{size}/220194_2.png\",\"created_at\":\"2024-04-22T00:39:48.309Z\",\"cooked\":\"\\u003cp\\u003eI used an Intel realsense D435 camera in 2023 to track 3d pose of gamepieces relative to the robot.\\u003c/p\\u003e\\n\\u003cp\\u003eIt’s a stero depth camera, and gived 3d coordinates to every pixle it sees, and don’t get affected by the sun as it’s not based on IR.\\u003c/p\\u003e\\n\\u003cp\\u003eIt might get integrated with a simple yolov8n and train it on recognizing bumpers, to get the 3d pose of other robots, but you’ll need a navidia gpu that supports cuda to do high fps.(google coral might also work)\\u003c/p\\u003e\\n\\u003cp\\u003eBut I think finding 3d robot pose not very useful on the effort-outcome spectrum. I think giving the driver more time to practice is more time efficient, and will have a better outcome. I don’t even think 3d gamepiece pose will be really effective if you have a wide touch-it-own-it intake\\u003c/p\\u003e\",\"post_number\":21,\"post_type\":1,\"posts_count\":33,\"updated_at\":\"2024-04-22T00:41:22.810Z\",\"reply_count\":1,\"reply_to_post_number\":null,\"quote_count\":0,\"incoming_link_count\":0,\"reads\":221,\"readers_count\":220,\"score\":49.2,\"yours\":false,\"topic_id\":463159,\"topic_slug\":\"using-vision-to-determine-other-robots-positions\",\"display_username\":\"Rocky\",\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_bg_color\":null,\"flair_color\":null,\"flair_group_id\":null,\"badges_granted\":[],\"version\":1,\"can_edit\":false,\"can_delete\":false,\"can_recover\":false,\"can_see_hidden_post\":true,\"can_wiki\":false,\"read\":true,\"user_title\":null,\"bookmarked\":false,\"actions_summary\":[],\"moderator\":false,\"admin\":false,\"staff\":false,\"user_id\":58611,\"hidden\":false,\"trust_level\":2,\"deleted_at\":null,\"user_deleted\":false,\"edit_reason\":null,\"can_view_edit_history\":true,\"wiki\":false,\"user_custom_fields\":{\"user_field_4\":\"\"},\"post_url\":\"/t/using-vision-to-determine-other-robots-positions/463159/21\",\"custom_user_title\":null,\"reactions\":[],\"current_user_reaction\":null,\"reaction_users_count\":0,\"current_user_used_main_reaction\":false,\"can_accept_answer\":false,\"can_unaccept_answer\":false,\"accepted_answer\":false,\"topic_accepted_answer\":null}],\"stream\":[3364383,3364399,3364408,3364420,3364451,3364461,3364467,3364487,3364929,3365496,3365523,3365528,3365534,3365540,3365568,3365578,3365858,3365859,3368711,3368721,3368751,3368758,3368814,3368839,3369516,3369718,3369821,3370520,3372002,3372423,3373825,3374142,3604374]},\"timeline_lookup\":[[1,542],[9,541],[17,540],[19,538],[25,537],[29,536],[30,535],[31,534],[33,169]],\"tags\":[\"robot\",\"vision\",\"ai\"],\"tags_descriptions\":{},\"fancy_title\":\"Using Vision to Determine Other Robots\\u0026rsquo; Positions\",\"id\":463159,\"title\":\"Using Vision to Determine Other Robots' Positions\",\"posts_count\":33,\"created_at\":\"2024-04-17T17:42:32.244Z\",\"views\":2748,\"reply_count\":23,\"like_count\":66,\"last_posted_at\":\"2025-04-26T03:16:26.136Z\",\"visible\":true,\"closed\":true,\"archived\":false,\"has_summary\":false,\"archetype\":\"regular\",\"slug\":\"using-vision-to-determine-other-robots-positions\",\"category_id\":25,\"word_count\":5255,\"deleted_at\":null,\"user_id\":54317,\"featured_link\":null,\"pinned_globally\":false,\"pinned_at\":null,\"pinned_until\":null,\"image_url\":null,\"slow_mode_seconds\":0,\"draft\":null,\"draft_key\":\"topic_463159\",\"draft_sequence\":null,\"unpinned\":null,\"pinned\":false,\"current_post_number\":1,\"highest_post_number\":34,\"deleted_by\":null,\"actions_summary\":[{\"id\":4,\"count\":0,\"hidden\":false,\"can_act\":false},{\"id\":8,\"count\":0,\"hidden\":false,\"can_act\":false},{\"id\":10,\"count\":0,\"hidden\":false,\"can_act\":false},{\"id\":7,\"count\":0,\"hidden\":false,\"can_act\":false}],\"chunk_size\":20,\"bookmarked\":false,\"topic_timer\":null,\"message_bus_last_id\":0,\"participant_count\":13,\"show_read_indicator\":false,\"thumbnails\":null,\"slow_mode_enabled_until\":null,\"valid_reactions\":[\"heart\",\"laughing\",\"point_up\",\"+1\",\"-1\",\"100\",\"open_mouth\",\"cry\",\"question\",\"thinking\",\"call_me_hand\",\"hugs\",\"angry\"],\"user_chosen_thumbnail_url\":null,\"sidecar_installed\":true,\"can_vote\":false,\"vote_count\":0,\"user_voted\":false,\"discourse_zendesk_plugin_zendesk_id\":null,\"discourse_zendesk_plugin_zendesk_url\":\"https://your-url.zendesk.com/agent/tickets/\",\"details\":{\"can_edit\":false,\"notification_level\":1,\"participants\":[{\"id\":53160,\"username\":\"RomanTechPlus\",\"name\":\"Roman A.\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/romantechplus/{size}/214929_2.png\",\"post_count\":10,\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_color\":null,\"flair_bg_color\":null,\"flair_group_id\":null,\"trust_level\":2},{\"id\":4297,\"username\":\"Andrew_Schreiber\",\"name\":\"Andrew Schreiber\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/andrew_schreiber/{size}/209563_2.png\",\"post_count\":5,\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_color\":null,\"flair_bg_color\":null,\"flair_group_id\":null,\"trust_level\":2},{\"id\":24178,\"username\":\"jdao\",\"name\":\"jdao\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/jdao/{size}/160969_2.png\",\"post_count\":3,\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_color\":null,\"flair_bg_color\":null,\"flair_group_id\":null,\"trust_level\":2},{\"id\":58611,\"username\":\"Rocky_S\",\"name\":\"Rocky\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/rocky_s/{size}/220194_2.png\",\"post_count\":2,\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_color\":null,\"flair_bg_color\":null,\"flair_group_id\":null,\"trust_level\":2},{\"id\":23786,\"username\":\"EKM_319\",\"name\":\"Eric (319)\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/ekm_319/{size}/230932_2.png\",\"post_count\":2,\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_color\":null,\"flair_bg_color\":null,\"flair_group_id\":null,\"trust_level\":2},{\"id\":25265,\"username\":\"badinkajink\",\"name\":\"William Xie\",\"avatar_template\":\"/letter_avatar_proxy/v4/letter/b/bbe5ce/{size}.png\",\"post_count\":2,\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_color\":null,\"flair_bg_color\":null,\"flair_group_id\":null,\"trust_level\":2},{\"id\":54317,\"username\":\"SirFire\",\"name\":\"\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/sirfire/{size}/222590_2.png\",\"post_count\":2,\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_color\":null,\"flair_bg_color\":null,\"flair_group_id\":null,\"trust_level\":2},{\"id\":8103,\"username\":\"Footie\",\"name\":\"Nick Foote\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/footie/{size}/218325_2.png\",\"post_count\":1,\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_color\":null,\"flair_bg_color\":null,\"flair_group_id\":null,\"trust_level\":2},{\"id\":26824,\"username\":\"BenBernard\",\"name\":\"Ben Bernard (He/Him)\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/benbernard/{size}/8721_2.png\",\"post_count\":1,\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_color\":null,\"flair_bg_color\":null,\"flair_group_id\":null,\"trust_level\":2},{\"id\":49892,\"username\":\"nobody5690\",\"name\":\"Levi\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/nobody5690/{size}/203585_2.png\",\"post_count\":1,\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_color\":null,\"flair_bg_color\":null,\"flair_group_id\":null,\"trust_level\":2},{\"id\":13575,\"username\":\"Peter_Johnson\",\"name\":\"Peter Johnson\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/peter_johnson/{size}/10417_2.png\",\"post_count\":1,\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_color\":null,\"flair_bg_color\":null,\"flair_group_id\":null,\"trust_level\":2},{\"id\":20003,\"username\":\"joemost\",\"name\":\"joemost\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/joemost/{size}/11114_2.png\",\"post_count\":1,\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_color\":null,\"flair_bg_color\":null,\"flair_group_id\":null,\"trust_level\":2},{\"id\":24617,\"username\":\"gerthworm\",\"name\":\"Chris Gerth\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/gerthworm/{size}/268658_2.png\",\"post_count\":1,\"primary_group_name\":null,\"flair_name\":null,\"flair_url\":null,\"flair_color\":null,\"flair_bg_color\":null,\"flair_group_id\":null,\"trust_level\":2}],\"created_by\":{\"id\":54317,\"username\":\"SirFire\",\"name\":\"\",\"avatar_template\":\"/user_avatar/www.chiefdelphi.com/sirfire/{size}/222590_2.png\"},\"last_poster\":{\"id\":-1,\"username\":\"system\",\"name\":\"system\",\"avatar_template\":\"/uploads/default/original/4X/9/3/2/93234578e69f55873c63fc00fe2d0d85f23bce55.png\"},\"links\":[{\"url\":\"https://github.com/schreiaj/frc-virtual-training-environment\",\"title\":\"GitHub - schreiaj/frc-virtual-training-environment\",\"internal\":false,\"attachment\":false,\"reflection\":false,\"clicks\":16,\"user_id\":4297,\"domain\":\"github.com\",\"root_domain\":\"github.com\"},{\"url\":\"https://www.youtube.com/watch?v=p-PIHN8kKPE\",\"title\":null,\"internal\":false,\"attachment\":false,\"reflection\":false,\"clicks\":12,\"user_id\":13575,\"domain\":\"www.youtube.com\",\"root_domain\":\"youtube.com\"},{\"url\":\"https://github.com/isl-org/ZoeDepth\",\"title\":\"GitHub - isl-org/ZoeDepth: Metric depth estimation from a single image\",\"internal\":false,\"attachment\":false,\"reflection\":false,\"clicks\":8,\"user_id\":4297,\"domain\":\"github.com\",\"root_domain\":\"github.com\"},{\"url\":\"https://www.tesla.com/autopilot\",\"title\":null,\"internal\":false,\"attachment\":false,\"reflection\":false,\"clicks\":6,\"user_id\":54317,\"domain\":\"www.tesla.com\",\"root_domain\":\"tesla.com\"},{\"url\":\"https://docs.luxonis.com/projects/api/en/latest/components/nodes/object_tracker/\",\"title\":\"ObjectTracker — DepthAI documentation | Luxonis\",\"internal\":false,\"attachment\":false,\"reflection\":false,\"clicks\":5,\"user_id\":4297,\"domain\":\"docs.luxonis.com\",\"root_domain\":\"luxonis.com\"},{\"url\":\"https://www.youtube.com/watch?v=RWRJYuvI6M4\",\"title\":null,\"internal\":false,\"attachment\":false,\"reflection\":false,\"clicks\":4,\"user_id\":4297,\"domain\":\"www.youtube.com\",\"root_domain\":\"youtube.com\"},{\"url\":\"https://docs.luxonis.com/projects/hardware/en/latest/pages/rvc/rvc2/#rvc2-nn-performance\",\"title\":\"Robotics Vision Core 2 (RVC2) — DepthAI Hardware Documentation 1.0.0 documentation\",\"internal\":false,\"attachment\":false,\"reflection\":false,\"clicks\":3,\"user_id\":24178,\"domain\":\"docs.luxonis.com\",\"root_domain\":\"luxonis.com\"},{\"url\":\"https://pypi.org/project/bytetracker/\",\"title\":\"bytetracker · PyPI\",\"internal\":false,\"attachment\":false,\"reflection\":false,\"clicks\":2,\"user_id\":4297,\"domain\":\"pypi.org\",\"root_domain\":\"pypi.org\"},{\"url\":\"https://docs.luxonis.com/projects/sdk/en/latest/samples/NNComponent/sdk_custom_decode/\",\"title\":\"Custom Decode Function — DepthAI SDK Docs 1.13.1 documentation\",\"internal\":false,\"attachment\":false,\"reflection\":false,\"clicks\":2,\"user_id\":24178,\"domain\":\"docs.luxonis.com\",\"root_domain\":\"luxonis.com\"},{\"url\":\"https://arxiv.org/abs/2311.15707\",\"title\":\"[2311.15707] SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation\",\"internal\":false,\"attachment\":false,\"reflection\":false,\"clicks\":2,\"user_id\":25265,\"domain\":\"arxiv.org\",\"root_domain\":\"arxiv.org\"}]},\"bookmarks\":[]}"}
