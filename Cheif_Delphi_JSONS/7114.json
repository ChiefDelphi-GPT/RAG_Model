{
    "data": {
        "post_stream": {
            "posts": [
                {
                    "id": 3346072,
                    "name": "Emil",
                    "username": "GronkBaby",
                    "avatar_template": "/letter_avatar_proxy/v4/letter/g/ba8739/{size}.png",
                    "created_at": "2024-03-28T07:14:32.111Z",
                    "cooked": "I’ve already seen a thread about this, but it was in 2018. I’m wondering if things have changed since then. I understand that teams can use paths on the fly between predefined points - but I feel like it can be more optimized. For example, an RF model could take  future cycles into account when making a path to the speaker to shoot, and how slight errors in angle, rpm, height (etc.) impact shots from different distances. Although, this is probably simple enough that pathplanning or just aligning would be almost as effective, for more complicated tasks I feel like it could be useful.\n\nThoughts?",
                    "post_number": 1,
                    "post_type": 1,
                    "posts_count": 8,
                    "updated_at": "2024-03-28T07:14:32.111Z",
                    "reply_count": 0,
                    "reply_to_post_number": null,
                    "quote_count": 0,
                    "incoming_link_count": 37,
                    "reads": 241,
                    "readers_count": 240,
                    "score": 263.2,
                    "yours": false,
                    "topic_id": 460436,
                    "topic_slug": "thoughts-on-deep-reinforcement-learning-for-optimized-paths",
                    "display_username": "Emil",
                    "primary_group_name": null,
                    "flair_name": null,
                    "flair_url": null,
                    "flair_bg_color": null,
                    "flair_color": null,
                    "flair_group_id": null,
                    "badges_granted": [],
                    "version": 1,
                    "can_edit": false,
                    "can_delete": false,
                    "can_recover": false,
                    "can_see_hidden_post": true,
                    "can_wiki": false,
                    "read": true,
                    "user_title": "",
                    "bookmarked": false,
                    "actions_summary": [
                        {
                            "id": 2,
                            "count": 2
                        }
                    ],
                    "moderator": false,
                    "admin": false,
                    "staff": false,
                    "user_id": 64351,
                    "hidden": false,
                    "trust_level": 2,
                    "deleted_at": null,
                    "user_deleted": false,
                    "edit_reason": null,
                    "can_view_edit_history": true,
                    "wiki": false,
                    "user_custom_fields": {
                        "user_field_4": ""
                    },
                    "post_url": "/t/thoughts-on-deep-reinforcement-learning-for-optimized-paths/460436/1",
                    "custom_user_title": null,
                    "reactions": [
                        {
                            "id": "heart",
                            "type": "emoji",
                            "count": 2
                        }
                    ],
                    "current_user_reaction": null,
                    "reaction_users_count": 2,
                    "current_user_used_main_reaction": false,
                    "can_accept_answer": false,
                    "can_unaccept_answer": false,
                    "accepted_answer": false,
                    "topic_accepted_answer": null,
                    "can_vote": false
                },
                {
                    "id": 3346077,
                    "name": "Rocky",
                    "username": "Rocky_S",
                    "avatar_template": "/user_avatar/www.chiefdelphi.com/rocky_s/{size}/220194_2.png",
                    "created_at": "2024-03-28T09:38:40.938Z",
                    "cooked": "I saw 1323’s auto in Central Valley Regional, they have Vision for note tracking that allows them to counter defensive auto in the midline in auto period, and pick up misplaced-notes. I think they might also have on-the-fly path planning to adjust their auto paths in the auto period.\n\nI think the endgoal of FRC is real-time SLAM and virtual “walls” and even LIDAR cameras/Depth cameras widely used, to allow the robot do pure auto cycles through a whole game.\n\nBut for now, with no open-source solutions available, it’s really hard to do, and I doubt taking so much time developing such a system is worth more than giving driver more time to practice.",
                    "post_number": 2,
                    "post_type": 1,
                    "posts_count": 8,
                    "updated_at": "2024-03-28T09:38:40.938Z",
                    "reply_count": 1,
                    "reply_to_post_number": null,
                    "quote_count": 0,
                    "incoming_link_count": 0,
                    "reads": 212,
                    "readers_count": 211,
                    "score": 77.4,
                    "yours": false,
                    "topic_id": 460436,
                    "topic_slug": "thoughts-on-deep-reinforcement-learning-for-optimized-paths",
                    "display_username": "Rocky",
                    "primary_group_name": null,
                    "flair_name": null,
                    "flair_url": null,
                    "flair_bg_color": null,
                    "flair_color": null,
                    "flair_group_id": null,
                    "badges_granted": [],
                    "version": 1,
                    "can_edit": false,
                    "can_delete": false,
                    "can_recover": false,
                    "can_see_hidden_post": true,
                    "can_wiki": false,
                    "read": true,
                    "user_title": null,
                    "bookmarked": false,
                    "actions_summary": [
                        {
                            "id": 2,
                            "count": 2
                        }
                    ],
                    "moderator": false,
                    "admin": false,
                    "staff": false,
                    "user_id": 58611,
                    "hidden": false,
                    "trust_level": 2,
                    "deleted_at": null,
                    "user_deleted": false,
                    "edit_reason": null,
                    "can_view_edit_history": true,
                    "wiki": false,
                    "user_custom_fields": {
                        "user_field_4": ""
                    },
                    "post_url": "/t/thoughts-on-deep-reinforcement-learning-for-optimized-paths/460436/2",
                    "custom_user_title": null,
                    "reactions": [
                        {
                            "id": "heart",
                            "type": "emoji",
                            "count": 2
                        }
                    ],
                    "current_user_reaction": null,
                    "reaction_users_count": 2,
                    "current_user_used_main_reaction": false,
                    "can_accept_answer": false,
                    "can_unaccept_answer": false,
                    "accepted_answer": false,
                    "topic_accepted_answer": null
                },
                {
                    "id": 3346084,
                    "name": "",
                    "username": "aven",
                    "avatar_template": "/user_avatar/www.chiefdelphi.com/aven/{size}/261990_2.png",
                    "created_at": "2024-03-28T10:25:32.963Z",
                    "cooked": "I’ve heard of teams (specifically 2471) using note detection so it could automatically go to notes in auto, and grab notes even if it missed the first attempt. I don’t think this used any rf models, but it would be worth asking them",
                    "post_number": 3,
                    "post_type": 1,
                    "posts_count": 8,
                    "updated_at": "2024-03-28T10:25:32.963Z",
                    "reply_count": 1,
                    "reply_to_post_number": null,
                    "quote_count": 0,
                    "incoming_link_count": 0,
                    "reads": 193,
                    "readers_count": 192,
                    "score": 43.6,
                    "yours": false,
                    "topic_id": 460436,
                    "topic_slug": "thoughts-on-deep-reinforcement-learning-for-optimized-paths",
                    "display_username": "",
                    "primary_group_name": null,
                    "flair_name": null,
                    "flair_url": null,
                    "flair_bg_color": null,
                    "flair_color": null,
                    "flair_group_id": null,
                    "badges_granted": [],
                    "version": 1,
                    "can_edit": false,
                    "can_delete": false,
                    "can_recover": false,
                    "can_see_hidden_post": true,
                    "can_wiki": false,
                    "read": true,
                    "user_title": null,
                    "bookmarked": false,
                    "actions_summary": [],
                    "moderator": false,
                    "admin": false,
                    "staff": false,
                    "user_id": 62505,
                    "hidden": false,
                    "trust_level": 2,
                    "deleted_at": null,
                    "user_deleted": false,
                    "edit_reason": null,
                    "can_view_edit_history": true,
                    "wiki": false,
                    "user_custom_fields": {
                        "user_field_4": "2374 strategy/vice design lead"
                    },
                    "post_url": "/t/thoughts-on-deep-reinforcement-learning-for-optimized-paths/460436/3",
                    "custom_user_title": "2374 strategy/vice design lead",
                    "reactions": [],
                    "current_user_reaction": null,
                    "reaction_users_count": 0,
                    "current_user_used_main_reaction": false,
                    "can_accept_answer": false,
                    "can_unaccept_answer": false,
                    "accepted_answer": false,
                    "topic_accepted_answer": null
                },
                {
                    "id": 3346096,
                    "name": "Jainish Patel",
                    "username": "JaiCodes",
                    "avatar_template": "/user_avatar/www.chiefdelphi.com/jaicodes/{size}/203987_2.png",
                    "created_at": "2024-03-28T11:13:39.988Z",
                    "cooked": "I feel like an RF model is not necessarily needed for this game to do full auto cycles.  Angle, RPM , and shooting distance can be predefined using regression like a lot of teams already do. We can use the A* algorithm to avoid game structure and potentially robots. We can have orange pis with cameras running a model to detect robots and notes, datasetcolab.com already has some. The path has to be regenerated in a loop to account for other robot movements (a Rio probably can’t do this). Teams such as 1690 have note position estimation, we can use this algorithm to define the end point of the A* path when going to the source and the starting point could be the speaker or amp and vice versa to go back. Only thing that RF model is applicable is for endgame so the robot can figure out on what to hang on depending on the alliance’s robots decision, picking up notes not near the source zones and what to score in. I feel like this is doable and I plan to do this in offseason.",
                    "post_number": 4,
                    "post_type": 1,
                    "posts_count": 8,
                    "updated_at": "2024-03-28T12:32:35.723Z",
                    "reply_count": 0,
                    "reply_to_post_number": null,
                    "quote_count": 0,
                    "incoming_link_count": 1,
                    "reads": 177,
                    "readers_count": 176,
                    "score": 40.4,
                    "yours": false,
                    "topic_id": 460436,
                    "topic_slug": "thoughts-on-deep-reinforcement-learning-for-optimized-paths",
                    "display_username": "Jainish Patel",
                    "primary_group_name": null,
                    "flair_name": null,
                    "flair_url": null,
                    "flair_bg_color": null,
                    "flair_color": null,
                    "flair_group_id": null,
                    "badges_granted": [],
                    "version": 2,
                    "can_edit": false,
                    "can_delete": false,
                    "can_recover": false,
                    "can_see_hidden_post": true,
                    "can_wiki": false,
                    "link_counts": [
                        {
                            "url": "http://datasetcolab.com",
                            "internal": false,
                            "reflection": false,
                            "title": "Dataset Colab",
                            "clicks": 6
                        }
                    ],
                    "read": true,
                    "user_title": "",
                    "bookmarked": false,
                    "actions_summary": [],
                    "moderator": false,
                    "admin": false,
                    "staff": false,
                    "user_id": 54172,
                    "hidden": false,
                    "trust_level": 2,
                    "deleted_at": null,
                    "user_deleted": false,
                    "edit_reason": null,
                    "can_view_edit_history": true,
                    "wiki": false,
                    "user_custom_fields": {
                        "user_field_4": "4065 NOP | Programming Lead"
                    },
                    "post_url": "/t/thoughts-on-deep-reinforcement-learning-for-optimized-paths/460436/4",
                    "custom_user_title": "4065 NOP | Programming Lead",
                    "reactions": [],
                    "current_user_reaction": null,
                    "reaction_users_count": 0,
                    "current_user_used_main_reaction": false,
                    "can_accept_answer": false,
                    "can_unaccept_answer": false,
                    "accepted_answer": false,
                    "topic_accepted_answer": null
                },
                {
                    "id": 3346124,
                    "name": "UnofficialForth",
                    "username": "UnofficialForth",
                    "avatar_template": "/user_avatar/www.chiefdelphi.com/unofficialforth/{size}/167902_2.png",
                    "created_at": "2024-03-28T12:28:44.310Z",
                    "cooked": "I like the conceptual idea of this, and it can be and will be eventually implemented for pathing.\n\nWith regards to shooting angles, you need a way to get back data about the shoot as well as if it made it or not, and you need a lot of it. It might be simpler to create a sort of interpolation map that allows for taking a smaller set of tests and try to extrapolate information about in between data points.",
                    "post_number": 5,
                    "post_type": 1,
                    "posts_count": 8,
                    "updated_at": "2024-03-28T12:28:44.310Z",
                    "reply_count": 0,
                    "reply_to_post_number": null,
                    "quote_count": 0,
                    "incoming_link_count": 0,
                    "reads": 137,
                    "readers_count": 136,
                    "score": 27.4,
                    "yours": false,
                    "topic_id": 460436,
                    "topic_slug": "thoughts-on-deep-reinforcement-learning-for-optimized-paths",
                    "display_username": "UnofficialForth",
                    "primary_group_name": null,
                    "flair_name": null,
                    "flair_url": null,
                    "flair_bg_color": null,
                    "flair_color": null,
                    "flair_group_id": null,
                    "badges_granted": [],
                    "version": 1,
                    "can_edit": false,
                    "can_delete": false,
                    "can_recover": false,
                    "can_see_hidden_post": true,
                    "can_wiki": false,
                    "read": true,
                    "user_title": "",
                    "bookmarked": false,
                    "actions_summary": [
                        {
                            "id": 2,
                            "count": 1
                        }
                    ],
                    "moderator": false,
                    "admin": false,
                    "staff": false,
                    "user_id": 29982,
                    "hidden": false,
                    "trust_level": 2,
                    "deleted_at": null,
                    "user_deleted": false,
                    "edit_reason": null,
                    "can_view_edit_history": true,
                    "wiki": false,
                    "user_custom_fields": {
                        "user_field_4": "FRC Team 4272  Mentor | World's FLL Emcee"
                    },
                    "post_url": "/t/thoughts-on-deep-reinforcement-learning-for-optimized-paths/460436/5",
                    "custom_user_title": "FRC Team 4272  Mentor | World's FLL Emcee",
                    "reactions": [
                        {
                            "id": "point_up",
                            "type": "emoji",
                            "count": 1
                        }
                    ],
                    "current_user_reaction": null,
                    "reaction_users_count": 1,
                    "current_user_used_main_reaction": false,
                    "can_accept_answer": false,
                    "can_unaccept_answer": false,
                    "accepted_answer": false,
                    "topic_accepted_answer": null
                },
                {
                    "id": 3346145,
                    "name": "Emil",
                    "username": "GronkBaby",
                    "avatar_template": "/letter_avatar_proxy/v4/letter/g/ba8739/{size}.png",
                    "created_at": "2024-03-28T13:09:01.703Z",
                    "cooked": "We used automatic intaking (during auto also) using Limelights. It’s not Reinforcement Learning, but it just uses a “pose calculation” of the note from the camera relative to the robot and defines PID based on that.  But you don’t even need that. You can use the detected note on the stream, and just drive in that direction while turning to face that position.",
                    "post_number": 6,
                    "post_type": 1,
                    "posts_count": 8,
                    "updated_at": "2024-03-28T13:09:01.703Z",
                    "reply_count": 0,
                    "reply_to_post_number": 3,
                    "quote_count": 0,
                    "incoming_link_count": 1,
                    "reads": 100,
                    "readers_count": 99,
                    "score": 40.0,
                    "yours": false,
                    "topic_id": 460436,
                    "topic_slug": "thoughts-on-deep-reinforcement-learning-for-optimized-paths",
                    "display_username": "Emil",
                    "primary_group_name": null,
                    "flair_name": null,
                    "flair_url": null,
                    "flair_bg_color": null,
                    "flair_color": null,
                    "flair_group_id": null,
                    "badges_granted": [],
                    "version": 1,
                    "can_edit": false,
                    "can_delete": false,
                    "can_recover": false,
                    "can_see_hidden_post": true,
                    "can_wiki": false,
                    "read": true,
                    "user_title": "",
                    "reply_to_user": {
                        "id": 62505,
                        "username": "aven",
                        "name": "",
                        "avatar_template": "/user_avatar/www.chiefdelphi.com/aven/{size}/261990_2.png"
                    },
                    "bookmarked": false,
                    "actions_summary": [
                        {
                            "id": 2,
                            "count": 1
                        }
                    ],
                    "moderator": false,
                    "admin": false,
                    "staff": false,
                    "user_id": 64351,
                    "hidden": false,
                    "trust_level": 2,
                    "deleted_at": null,
                    "user_deleted": false,
                    "edit_reason": null,
                    "can_view_edit_history": true,
                    "wiki": false,
                    "user_custom_fields": {
                        "user_field_4": ""
                    },
                    "post_url": "/t/thoughts-on-deep-reinforcement-learning-for-optimized-paths/460436/6",
                    "custom_user_title": null,
                    "reactions": [
                        {
                            "id": "heart",
                            "type": "emoji",
                            "count": 1
                        }
                    ],
                    "current_user_reaction": null,
                    "reaction_users_count": 1,
                    "current_user_used_main_reaction": false,
                    "can_accept_answer": false,
                    "can_unaccept_answer": false,
                    "accepted_answer": false,
                    "topic_accepted_answer": null
                },
                {
                    "id": 3346338,
                    "name": "Emil",
                    "username": "GronkBaby",
                    "avatar_template": "/letter_avatar_proxy/v4/letter/g/ba8739/{size}.png",
                    "created_at": "2024-03-28T16:14:30.225Z",
                    "cooked": "I doubt they use on-the-path planning. I don’t see the advantage if you can just drive in the direction of the note and rotate to it (which is as effective).",
                    "post_number": 7,
                    "post_type": 1,
                    "posts_count": 8,
                    "updated_at": "2024-03-28T16:14:30.225Z",
                    "reply_count": 0,
                    "reply_to_post_number": 2,
                    "quote_count": 0,
                    "incoming_link_count": 0,
                    "reads": 65,
                    "readers_count": 64,
                    "score": 28.0,
                    "yours": false,
                    "topic_id": 460436,
                    "topic_slug": "thoughts-on-deep-reinforcement-learning-for-optimized-paths",
                    "display_username": "Emil",
                    "primary_group_name": null,
                    "flair_name": null,
                    "flair_url": null,
                    "flair_bg_color": null,
                    "flair_color": null,
                    "flair_group_id": null,
                    "badges_granted": [],
                    "version": 1,
                    "can_edit": false,
                    "can_delete": false,
                    "can_recover": false,
                    "can_see_hidden_post": true,
                    "can_wiki": false,
                    "read": true,
                    "user_title": "",
                    "reply_to_user": {
                        "id": 58611,
                        "username": "Rocky_S",
                        "name": "Rocky",
                        "avatar_template": "/user_avatar/www.chiefdelphi.com/rocky_s/{size}/220194_2.png"
                    },
                    "bookmarked": false,
                    "actions_summary": [
                        {
                            "id": 2,
                            "count": 1
                        }
                    ],
                    "moderator": false,
                    "admin": false,
                    "staff": false,
                    "user_id": 64351,
                    "hidden": false,
                    "trust_level": 2,
                    "deleted_at": null,
                    "user_deleted": false,
                    "edit_reason": null,
                    "can_view_edit_history": true,
                    "wiki": false,
                    "user_custom_fields": {
                        "user_field_4": ""
                    },
                    "post_url": "/t/thoughts-on-deep-reinforcement-learning-for-optimized-paths/460436/7",
                    "custom_user_title": null,
                    "reactions": [
                        {
                            "id": "heart",
                            "type": "emoji",
                            "count": 1
                        }
                    ],
                    "current_user_reaction": null,
                    "reaction_users_count": 1,
                    "current_user_used_main_reaction": false,
                    "can_accept_answer": false,
                    "can_unaccept_answer": false,
                    "accepted_answer": false,
                    "topic_accepted_answer": null
                },
                {
                    "id": 3571336,
                    "name": "system",
                    "username": "system",
                    "avatar_template": "/uploads/default/original/4X/9/3/2/93234578e69f55873c63fc00fe2d0d85f23bce55.png",
                    "created_at": "2025-03-28T16:15:04.091Z",
                    "cooked": "This topic was automatically closed 365 days after the last reply. New replies are no longer allowed.",
                    "post_number": 8,
                    "post_type": 3,
                    "posts_count": 8,
                    "updated_at": "2025-03-28T16:15:04.091Z",
                    "reply_count": 0,
                    "reply_to_post_number": null,
                    "quote_count": 0,
                    "incoming_link_count": 0,
                    "reads": 4,
                    "readers_count": 3,
                    "score": 0.8,
                    "yours": false,
                    "topic_id": 460436,
                    "topic_slug": "thoughts-on-deep-reinforcement-learning-for-optimized-paths",
                    "display_username": "system",
                    "primary_group_name": null,
                    "flair_name": null,
                    "flair_url": null,
                    "flair_bg_color": null,
                    "flair_color": null,
                    "flair_group_id": null,
                    "badges_granted": [],
                    "version": 1,
                    "can_edit": false,
                    "can_delete": false,
                    "can_recover": false,
                    "can_see_hidden_post": true,
                    "can_wiki": false,
                    "read": true,
                    "user_title": "",
                    "bookmarked": false,
                    "actions_summary": [],
                    "moderator": true,
                    "admin": true,
                    "staff": true,
                    "user_id": -1,
                    "hidden": false,
                    "trust_level": 4,
                    "deleted_at": null,
                    "user_deleted": false,
                    "edit_reason": null,
                    "can_view_edit_history": true,
                    "wiki": false,
                    "action_code": "autoclosed.enabled",
                    "post_url": "/t/thoughts-on-deep-reinforcement-learning-for-optimized-paths/460436/8",
                    "custom_user_title": null,
                    "reactions": [],
                    "current_user_reaction": null,
                    "reaction_users_count": 0,
                    "current_user_used_main_reaction": false,
                    "can_accept_answer": false,
                    "can_unaccept_answer": false,
                    "accepted_answer": false,
                    "topic_accepted_answer": null
                }
            ],
            "stream": [
                3346072,
                3346077,
                3346084,
                3346096,
                3346124,
                3346145,
                3346338,
                3571336
            ]
        },
        "timeline_lookup": [
            [
                1,
                563
            ],
            [
                5,
                562
            ],
            [
                8,
                197
            ]
        ],
        "suggested_topics": [
            {
                "fancy_title": "Spark Flex limit switch not reading unless enabled",
                "id": 491111,
                "title": "Spark Flex limit switch not reading unless enabled",
                "slug": "spark-flex-limit-switch-not-reading-unless-enabled",
                "posts_count": 3,
                "reply_count": 0,
                "highest_post_number": 3,
                "image_url": null,
                "created_at": "2025-02-11T00:24:59.787Z",
                "last_posted_at": "2025-02-17T23:36:19.679Z",
                "bumped": true,
                "bumped_at": "2025-02-17T23:36:19.679Z",
                "archetype": "regular",
                "unseen": false,
                "pinned": false,
                "unpinned": null,
                "excerpt": "We have a limit switch connected to the forward limit switch input on our spark Flex to detect the coral in our coral mechanism. When the limit switch is enabled in the spark config in code, we can r…",
                "visible": true,
                "closed": false,
                "archived": false,
                "bookmarked": null,
                "liked": null,
                "tags": [],
                "tags_descriptions": {},
                "like_count": 4,
                "views": 171,
                "category_id": 30,
                "featured_link": null,
                "has_accepted_answer": false,
                "sidecar_installed": true,
                "include_dominant_colour": false,
                "topic_post_id": 3522885,
                "topic_post_liked": false,
                "topic_post_can_like": false,
                "topic_post_can_unlike": false,
                "topic_post_bookmarked": false,
                "topic_post_is_current_users": null,
                "topic_post_number": 1,
                "topic_post_user": {
                    "id": 6115,
                    "username": "Ty_Tremblay",
                    "name": "Ty Tremblay",
                    "avatar_template": "/user_avatar/www.chiefdelphi.com/ty_tremblay/{size}/219248_2.png"
                },
                "posters": [
                    {
                        "extras": null,
                        "description": "Original Poster",
                        "user": {
                            "id": 6115,
                            "username": "Ty_Tremblay",
                            "name": "Ty Tremblay",
                            "avatar_template": "/user_avatar/www.chiefdelphi.com/ty_tremblay/{size}/219248_2.png",
                            "trust_level": 2
                        }
                    },
                    {
                        "extras": null,
                        "description": "Frequent Poster",
                        "user": {
                            "id": 49261,
                            "username": "jan",
                            "name": "",
                            "avatar_template": "/user_avatar/www.chiefdelphi.com/jan/{size}/158811_2.png",
                            "primary_group_name": "REV",
                            "flair_name": "REV",
                            "flair_url": "/uploads/default/original/3X/d/b/db93216ec14c273ae2ca04aa396186c0461d0620.png",
                            "flair_bg_color": "F05A28",
                            "flair_group_id": 55,
                            "trust_level": 2
                        }
                    },
                    {
                        "extras": "latest",
                        "description": "Most Recent Poster",
                        "user": {
                            "id": 14283,
                            "username": "nuttle",
                            "name": "Allen Nuttle",
                            "avatar_template": "/user_avatar/www.chiefdelphi.com/nuttle/{size}/124105_2.png",
                            "trust_level": 2
                        }
                    }
                ]
            },
            {
                "fancy_title": "Does anyone know how to make the control vibrate with the sensor signals on the FRC robot using WPIlib?",
                "id": 474115,
                "title": "Does anyone know how to make the control vibrate with the sensor signals on the FRC robot using WPIlib?",
                "slug": "does-anyone-know-how-to-make-the-control-vibrate-with-the-sensor-signals-on-the-frc-robot-using-wpilib",
                "posts_count": 3,
                "reply_count": 0,
                "highest_post_number": 3,
                "image_url": null,
                "created_at": "2024-10-31T17:57:52.128Z",
                "last_posted_at": "2024-10-31T18:26:28.159Z",
                "bumped": true,
                "bumped_at": "2024-10-31T18:26:28.159Z",
                "archetype": "regular",
                "unseen": false,
                "pinned": false,
                "unpinned": null,
                "excerpt": "This is something I really want to do in my FRC robot. Even more so using sensors and computer vision together. For when the robot manages to align with the april tag of a target or objective…",
                "visible": true,
                "closed": false,
                "archived": false,
                "bookmarked": null,
                "liked": null,
                "tags": [
                    "robot"
                ],
                "tags_descriptions": {},
                "like_count": 4,
                "views": 163,
                "category_id": 30,
                "featured_link": null,
                "has_accepted_answer": false,
                "sidecar_installed": true,
                "include_dominant_colour": false,
                "topic_post_id": 3442215,
                "topic_post_liked": false,
                "topic_post_can_like": false,
                "topic_post_can_unlike": false,
                "topic_post_bookmarked": false,
                "topic_post_is_current_users": null,
                "topic_post_number": 1,
                "topic_post_user": {
                    "id": 67970,
                    "username": "Bruno",
                    "name": "",
                    "avatar_template": "/letter_avatar_proxy/v4/letter/b/c77e96/{size}.png"
                },
                "posters": [
                    {
                        "extras": null,
                        "description": "Original Poster",
                        "user": {
                            "id": 67970,
                            "username": "Bruno",
                            "name": "",
                            "avatar_template": "/letter_avatar_proxy/v4/letter/b/c77e96/{size}.png",
                            "trust_level": 0
                        }
                    },
                    {
                        "extras": null,
                        "description": "Frequent Poster",
                        "user": {
                            "id": 19820,
                            "username": "ArchdukeTim",
                            "name": "Tim Winters",
                            "avatar_template": "/user_avatar/www.chiefdelphi.com/archduketim/{size}/256125_2.png",
                            "trust_level": 2
                        }
                    },
                    {
                        "extras": "latest",
                        "description": "Most Recent Poster",
                        "user": {
                            "id": 23320,
                            "username": "Owen_Busler",
                            "name": "Owen Busler",
                            "avatar_template": "/user_avatar/www.chiefdelphi.com/owen_busler/{size}/7684_2.png",
                            "trust_level": 2
                        }
                    }
                ]
            },
            {
                "fancy_title": "RoboRIO Imaging",
                "id": 480127,
                "title": "RoboRIO Imaging",
                "slug": "roborio-imaging",
                "posts_count": 3,
                "reply_count": 0,
                "highest_post_number": 3,
                "image_url": null,
                "created_at": "2025-01-08T13:01:12.209Z",
                "last_posted_at": "2025-01-08T13:55:56.256Z",
                "bumped": true,
                "bumped_at": "2025-01-08T13:55:56.256Z",
                "archetype": "regular",
                "unseen": false,
                "pinned": false,
                "unpinned": null,
                "excerpt": "R701 is the answer. ROBOTS must be controlled via 1 programmable NI roboRIO or roboRIO 2.0 (P/N am3000 or am3000a, both versions referred to throughout this manual as “roboRIO”), with image version …",
                "visible": true,
                "closed": false,
                "archived": false,
                "bookmarked": null,
                "liked": null,
                "tags": [],
                "tags_descriptions": {},
                "like_count": 1,
                "views": 123,
                "category_id": 30,
                "featured_link": null,
                "has_accepted_answer": true,
                "sidecar_installed": true,
                "include_dominant_colour": false,
                "topic_post_id": 3475242,
                "topic_post_liked": false,
                "topic_post_can_like": false,
                "topic_post_can_unlike": false,
                "topic_post_bookmarked": false,
                "topic_post_is_current_users": null,
                "topic_post_number": 3,
                "topic_post_user": {
                    "id": 16147,
                    "username": "Thad_House",
                    "name": "Thad House",
                    "avatar_template": "/letter_avatar_proxy/v4/letter/t/a4c791/{size}.png"
                },
                "posters": [
                    {
                        "extras": null,
                        "description": "Original Poster",
                        "user": {
                            "id": 64486,
                            "username": "pixelpunk",
                            "name": null,
                            "avatar_template": "/letter_avatar_proxy/v4/letter/p/96bed5/{size}.png",
                            "trust_level": 2
                        }
                    },
                    {
                        "extras": "latest",
                        "description": "Most Recent Poster, Accepted Answer",
                        "user": {
                            "id": 16147,
                            "username": "Thad_House",
                            "name": "Thad House",
                            "avatar_template": "/letter_avatar_proxy/v4/letter/t/a4c791/{size}.png",
                            "trust_level": 2
                        }
                    },
                    {
                        "extras": null,
                        "description": "Frequent Poster",
                        "user": {
                            "id": 53274,
                            "username": "EmerqldWither",
                            "name": "",
                            "avatar_template": "/user_avatar/www.chiefdelphi.com/emerqldwither/{size}/226176_2.png",
                            "trust_level": 2
                        }
                    }
                ]
            },
            {
                "fancy_title": "Introducing BumbleBoard: A New FRC Dashboard",
                "id": 475122,
                "title": "Introducing BumbleBoard: A New FRC Dashboard",
                "slug": "introducing-bumbleboard-a-new-frc-dashboard",
                "posts_count": 26,
                "reply_count": 18,
                "highest_post_number": 29,
                "image_url": "https://www.chiefdelphi.com/uploads/default/optimized/3X/8/e/8e1eadffebdf1aaa5403b030267a3b6261416063_2_1024x552.jpeg",
                "created_at": "2024-11-18T21:18:41.026Z",
                "last_posted_at": "2025-02-12T13:31:37.549Z",
                "bumped": true,
                "bumped_at": "2025-02-12T13:31:37.549Z",
                "archetype": "regular",
                "unseen": false,
                "pinned": false,
                "unpinned": null,
                "excerpt": "After a year of development, Team BumbleB 3339 is excited to announce BumbleBoard — a high-performance, customizable, and reliable dashboard alternative built with React. We developed BumbleBoard …",
                "visible": true,
                "closed": false,
                "archived": false,
                "bookmarked": null,
                "liked": null,
                "tags": [
                    "robot",
                    "programming",
                    "field",
                    "pits",
                    "pit",
                    "dashboard",
                    "shuffleboard",
                    "visualization",
                    "drive-team"
                ],
                "tags_descriptions": {},
                "like_count": 297,
                "views": 2822,
                "category_id": 30,
                "featured_link": null,
                "has_accepted_answer": false,
                "sidecar_installed": true,
                "include_dominant_colour": false,
                "topic_post_id": 3448990,
                "topic_post_liked": false,
                "topic_post_like_count": 26,
                "topic_post_can_like": false,
                "topic_post_can_unlike": false,
                "topic_post_bookmarked": true,
                "topic_post_is_current_users": null,
                "topic_post_number": 1,
                "topic_post_user": {
                    "id": 68925,
                    "username": "Amit627",
                    "name": "Amit Cohen",
                    "avatar_template": "/user_avatar/www.chiefdelphi.com/amit627/{size}/236209_2.png"
                },
                "posters": [
                    {
                        "extras": "latest",
                        "description": "Original Poster, Most Recent Poster",
                        "user": {
                            "id": 68925,
                            "username": "Amit627",
                            "name": "Amit Cohen",
                            "avatar_template": "/user_avatar/www.chiefdelphi.com/amit627/{size}/236209_2.png",
                            "trust_level": 1
                        }
                    },
                    {
                        "extras": null,
                        "description": "Frequent Poster",
                        "user": {
                            "id": 16846,
                            "username": "calcmogul",
                            "name": "Tyler Veness",
                            "avatar_template": "/user_avatar/www.chiefdelphi.com/calcmogul/{size}/128045_2.png",
                            "trust_level": 2
                        }
                    },
                    {
                        "extras": null,
                        "description": "Frequent Poster",
                        "user": {
                            "id": 53190,
                            "username": "truher",
                            "name": "joel truher",
                            "avatar_template": "/user_avatar/www.chiefdelphi.com/truher/{size}/172222_2.png",
                            "trust_level": 2
                        }
                    },
                    {
                        "extras": null,
                        "description": "Frequent Poster",
                        "user": {
                            "id": 19235,
                            "username": "Skyehawk",
                            "name": "Skye Leake",
                            "avatar_template": "/user_avatar/www.chiefdelphi.com/skyehawk/{size}/179363_2.png",
                            "moderator": true,
                            "trust_level": 4
                        }
                    },
                    {
                        "extras": null,
                        "description": "Frequent Poster",
                        "user": {
                            "id": 60033,
                            "username": "catrix",
                            "name": "sam liu",
                            "avatar_template": "/letter_avatar_proxy/v4/letter/c/dbc845/{size}.png",
                            "trust_level": 2
                        }
                    }
                ]
            },
            {
                "fancy_title": "Limelight 3 message: &ldquo;Check accelerator&rdquo;",
                "id": 494374,
                "title": "Limelight 3 message: \"Check accelerator\"",
                "slug": "limelight-3-message-check-accelerator",
                "posts_count": 8,
                "reply_count": 5,
                "highest_post_number": 8,
                "image_url": "https://www.chiefdelphi.com/uploads/default/original/3X/7/0/702a91c2f899e0271d3961153853bdeb50070943.png",
                "created_at": "2025-03-05T18:24:31.294Z",
                "last_posted_at": "2025-03-05T19:06:45.080Z",
                "bumped": true,
                "bumped_at": "2025-03-05T19:06:45.080Z",
                "archetype": "regular",
                "unseen": false,
                "pinned": false,
                "unpinned": null,
                "excerpt": "Our limelight 3 is hooked up to a google coral right now, that I’m pretty sure works since we used it last season without issue. Here’s what I’m seeing on the Im not sure what it means but…",
                "visible": true,
                "closed": false,
                "archived": false,
                "bookmarked": null,
                "liked": null,
                "tags": [],
                "tags_descriptions": {},
                "like_count": 1,
                "views": 184,
                "category_id": 30,
                "featured_link": null,
                "has_accepted_answer": false,
                "sidecar_installed": true,
                "include_dominant_colour": false,
                "topic_post_id": 3548446,
                "topic_post_liked": false,
                "topic_post_can_like": false,
                "topic_post_can_unlike": false,
                "topic_post_bookmarked": false,
                "topic_post_is_current_users": null,
                "topic_post_number": 1,
                "topic_post_user": {
                    "id": 64351,
                    "username": "GronkBaby",
                    "name": "Emil",
                    "avatar_template": "/letter_avatar_proxy/v4/letter/g/ba8739/{size}.png"
                },
                "posters": [
                    {
                        "extras": null,
                        "description": "Original Poster",
                        "user": {
                            "id": 64351,
                            "username": "GronkBaby",
                            "name": "Emil",
                            "avatar_template": "/letter_avatar_proxy/v4/letter/g/ba8739/{size}.png",
                            "trust_level": 2
                        }
                    },
                    {
                        "extras": null,
                        "description": "Frequent Poster",
                        "user": {
                            "id": 50762,
                            "username": "atsekhan",
                            "name": "Alex Tsekhansky",
                            "avatar_template": "/letter_avatar_proxy/v4/letter/a/0ea827/{size}.png",
                            "trust_level": 2
                        }
                    },
                    {
                        "extras": "latest",
                        "description": "Most Recent Poster",
                        "user": {
                            "id": 11043,
                            "username": "kgzak",
                            "name": "kgzak",
                            "avatar_template": "/user_avatar/www.chiefdelphi.com/kgzak/{size}/157079_2.png",
                            "trust_level": 2
                        }
                    }
                ]
            }
        ],
        "tags": [],
        "tags_descriptions": {},
        "fancy_title": "Thoughts on Deep Reinforcement Learning for Optimized Paths",
        "id": 460436,
        "title": "Thoughts on Deep Reinforcement Learning for Optimized Paths",
        "posts_count": 8,
        "created_at": "2024-03-28T07:14:32.060Z",
        "views": 507,
        "reply_count": 2,
        "like_count": 7,
        "last_posted_at": "2025-03-28T16:15:04.091Z",
        "visible": true,
        "closed": true,
        "archived": false,
        "has_summary": false,
        "archetype": "regular",
        "slug": "thoughts-on-deep-reinforcement-learning-for-optimized-paths",
        "category_id": 30,
        "word_count": 656,
        "deleted_at": null,
        "user_id": 64351,
        "featured_link": null,
        "pinned_globally": false,
        "pinned_at": null,
        "pinned_until": null,
        "image_url": null,
        "slow_mode_seconds": 0,
        "draft": null,
        "draft_key": "topic_460436",
        "draft_sequence": null,
        "unpinned": null,
        "pinned": false,
        "current_post_number": 1,
        "highest_post_number": 8,
        "deleted_by": null,
        "actions_summary": [
            {
                "id": 4,
                "count": 0,
                "hidden": false,
                "can_act": false
            },
            {
                "id": 8,
                "count": 0,
                "hidden": false,
                "can_act": false
            },
            {
                "id": 10,
                "count": 0,
                "hidden": false,
                "can_act": false
            },
            {
                "id": 7,
                "count": 0,
                "hidden": false,
                "can_act": false
            }
        ],
        "chunk_size": 20,
        "bookmarked": false,
        "topic_timer": null,
        "message_bus_last_id": 0,
        "participant_count": 5,
        "show_read_indicator": false,
        "thumbnails": null,
        "slow_mode_enabled_until": null,
        "valid_reactions": [
            "heart",
            "laughing",
            "point_up",
            "+1",
            "-1",
            "100",
            "open_mouth",
            "cry",
            "question",
            "thinking",
            "call_me_hand",
            "hugs",
            "angry"
        ],
        "user_chosen_thumbnail_url": null,
        "sidecar_installed": true,
        "can_vote": false,
        "vote_count": 0,
        "user_voted": false,
        "discourse_zendesk_plugin_zendesk_id": null,
        "discourse_zendesk_plugin_zendesk_url": "https://your-url.zendesk.com/agent/tickets/",
        "details": {
            "can_edit": false,
            "notification_level": 1,
            "participants": [
                {
                    "id": 64351,
                    "username": "GronkBaby",
                    "name": "Emil",
                    "avatar_template": "/letter_avatar_proxy/v4/letter/g/ba8739/{size}.png",
                    "post_count": 3,
                    "primary_group_name": null,
                    "flair_name": null,
                    "flair_url": null,
                    "flair_color": null,
                    "flair_bg_color": null,
                    "flair_group_id": null,
                    "trust_level": 2
                },
                {
                    "id": 29982,
                    "username": "UnofficialForth",
                    "name": "UnofficialForth",
                    "avatar_template": "/user_avatar/www.chiefdelphi.com/unofficialforth/{size}/167902_2.png",
                    "post_count": 1,
                    "primary_group_name": null,
                    "flair_name": null,
                    "flair_url": null,
                    "flair_color": null,
                    "flair_bg_color": null,
                    "flair_group_id": null,
                    "trust_level": 2
                },
                {
                    "id": 54172,
                    "username": "JaiCodes",
                    "name": "Jainish Patel",
                    "avatar_template": "/user_avatar/www.chiefdelphi.com/jaicodes/{size}/203987_2.png",
                    "post_count": 1,
                    "primary_group_name": null,
                    "flair_name": null,
                    "flair_url": null,
                    "flair_color": null,
                    "flair_bg_color": null,
                    "flair_group_id": null,
                    "trust_level": 2
                },
                {
                    "id": 58611,
                    "username": "Rocky_S",
                    "name": "Rocky",
                    "avatar_template": "/user_avatar/www.chiefdelphi.com/rocky_s/{size}/220194_2.png",
                    "post_count": 1,
                    "primary_group_name": null,
                    "flair_name": null,
                    "flair_url": null,
                    "flair_color": null,
                    "flair_bg_color": null,
                    "flair_group_id": null,
                    "trust_level": 2
                },
                {
                    "id": 62505,
                    "username": "aven",
                    "name": "",
                    "avatar_template": "/user_avatar/www.chiefdelphi.com/aven/{size}/261990_2.png",
                    "post_count": 1,
                    "primary_group_name": null,
                    "flair_name": null,
                    "flair_url": null,
                    "flair_color": null,
                    "flair_bg_color": null,
                    "flair_group_id": null,
                    "trust_level": 2
                }
            ],
            "created_by": {
                "id": 64351,
                "username": "GronkBaby",
                "name": "Emil",
                "avatar_template": "/letter_avatar_proxy/v4/letter/g/ba8739/{size}.png"
            },
            "last_poster": {
                "id": -1,
                "username": "system",
                "name": "system",
                "avatar_template": "/uploads/default/original/4X/9/3/2/93234578e69f55873c63fc00fe2d0d85f23bce55.png"
            },
            "links": [
                {
                    "url": "http://datasetcolab.com",
                    "title": "Dataset Colab",
                    "internal": false,
                    "attachment": false,
                    "reflection": false,
                    "clicks": 6,
                    "user_id": 54172,
                    "domain": "datasetcolab.com",
                    "root_domain": "datasetcolab.com"
                }
            ]
        },
        "bookmarks": []
    }
}